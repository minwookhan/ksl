* proto structure
#+begin_src proto
syntax = "proto3";
package vision.raw.v1;

service SequenceService {
  rpc SendFrames (stream Frame) returns (SubmitResultResponse);
}

message Point3 {
  float x = 1;
  float y = 2;
  float z = 3;
}

message Frame {
  string session_id = 1;
  int32 index = 2;
  int32 flag = 3; // 0: START, 1: NORMAL, 2: END
  int32 width = 4;
  int32 height = 5;
  int32 type = 6;
  bytes data = 7;
  repeated Point3 pose_points = 8;
}

message SubmitResultResponse {
  string session_id = 1;
  int32 frame_count = 2;
  string message = 3;
}
 session_id(string): 세션 식별자
  - index(int32): 프레임 번호
  - flag(int32): 상태 플래그(0=START, 1=NORMAL, 2=END)
  - width/height/type(int32): 프레임 크기와 OpenCV 타입 값
  - data(bytes): 프레임 이미지 바이너리
  - pose_points(repeated Point3): 포즈 좌표 목록(x,y,z float)

#+end_src
#+begin_src markdown
* ksl_sentence_recognition.proto` 파일에 정의된 데이터 구조 및 역할:**

1.  **`Point3` 메시지**
    *   **구조**: `float x`, `float y`, `float z` 세 개의 부동 소수점 필드를 가집니다.
    *   **역할**: 3차원 공간에서의 한 지점(예: MediaPipe로 추출된 스켈레톤 관절의 3D 좌표)을 표현하는 데 사용됩니다.

2.  **`Frame` 메시지**
    *   **구조**:
        *   `string session_id`: 현재 비디오 처리 세션을 식별하는 문자열 ID.
        *   `int32 index`: 비디오 시퀀스 내에서 현재 프레임의 순서 번호.
        *   `int32 flag`: 프레임의 특정 상태나 목적을 나타내는 정수 플래그 (예: 0=시작, 1=일반, 2=종료).
        *   `int32 width`: 프레임 이미지의 가로 픽셀 수.
        *   `int32 height`: 프레임 이미지의 세로 픽셀 수.
        *   `int32 type`: OpenCV `cv::Mat` 객체의 데이터 타입 (예: `CV_8UC3`은 8비트 unsigned char 3채널 이미지를 의미).
        *   `bytes data`: 실제 프레임 이미지의 바이너리 데이터 (압축되지 않은 픽셀 데이터).
        *   `repeated Point3 pose_points`: 이 프레임에서 감지된 여러 `Point3` 객체들의 목록. 주로 MediaPipe와 같은 AI 모델이 추출한 인체 관절의 3D 좌표를 담습니다.
    *   **역할**: 클라이언트가 gRPC 서버로 전송하는 단일 비디오 프레임에 대한 모든 관련 정보를 캡슐화합니다. 이미지 데이터 자체와 함께, 해당 프레임의 메타데이터(세션 ID, 인덱스, 플래그, 크기, 타입) 및 AI 분석 결과(포즈 포인트)를 포함하여 서버가 수어 인식을 수행하는 데 필요한 모든 컨텍스트를 제공합니다.

3.  **`SubmitResultResponse` 메시지**
    *   **구조**:
        *   `string session_id`: 요청 시 클라이언트가 보낸 세션 ID와 동일한 ID.
        *   `int32 frame_count`: 서버가 처리한 총 프레임 수.
        *   `string message`: 서버가 인식한 최종 수어 문장 텍스트 결과.
    *   **역할**: gRPC 서버가 클라이언트에게 수어 인식 처리 결과를 반환할 때 사용하는 메시지입니다. 어떤 세션에 대한 응답인지, 몇 개의 프레임이 처리되었는지, 그리고 가장 중요한 인식된 수어 문장이 무엇인지를 클라이언트에게 전달합니다.

#+end_src

* 전체 흐름
#+TITLE: 프로젝트 설명
** 1. 파일 개요
*** `gRPCFileClient.cpp` / `gRPCFileClient.h`
   이 파일들은 gRPC 클라이언트의 핵심 로직을 포함합니다. gRPC 서비스 호출을 오케스트레이션하고, 애플리케이션의 전반적인 상태 관리 및 비디오 처리와 gRPC 통신 간의 조정을 담당합니다.
*** `gRPCFileClientDlg.cpp` / `gRPCFileClientDlg.h`
   MFC(Microsoft Foundation Classes) 기반의 다이얼로그 및 UI 흐름을 정의합니다. 사용자 인터페이스를 관리하고, 사용자 입력(예: 파일 선택, 버튼 클릭)을 처리하며, gRPC 통신 결과를 화면에 표시하는 역할을 합니다.
*** `framework.h`
   MFC 및 Windows 애플리케이션의 공통 헤더 파일을 포함하여 프로젝트 전반에 걸쳐 필요한 기본 프레임워크 정의를 제공합니다.
*** `pch.cpp` / `pch.h`
   미리 컴파일된 헤더(Precompiled Header) 파일로, 빌드 시간을 단축하기 위해 자주 변경되지 않는 헤더들을 미리 컴파일하여 관리합니다.
*** `resource.h`
   Windows 애플리케이션의 UI 요소(아이콘, 메뉴, 다이얼로그 템플릿 등)에 대한 리소스 ID를 정의합니다.
*** `targetver.h`
   애플리케이션이 대상으로 하는 Windows 운영 체제 버전을 정의합니다.
*** `ksl_sentence_recognition.proto`
   gRPC 서비스의 계약(Service Contract)과 서버-클라이언트 간에 교환될 데이터 메시지 구조를 정의하는 프로토콜 버퍼 정의 파일입니다.
*** `ksl_sentence_recognition.grpc.pb.cc` / `ksl_sentence_recognition.grpc.pb.h`
   `ksl_sentence_recognition.proto` 파일로부터 자동으로 생성된 C++ 코드입니다. gRPC 클라이언트 스텁(Stub)과 서비스 인터페이스를 제공하여 서버와의 통신을 가능하게 합니다.
*** `ksl_sentence_recognition.pb.cc` / `ksl_sentence_recognition.pb.h`
   `ksl_sentence_recognition.proto` 파일로부터 자동으로 생성된 C++ 코드입니다. 프로토콜 버퍼 메시지 클래스들을 정의하여 데이터의 직렬화 및 역직렬화를 처리합니다.
*** `VideoUtil-v1.1.cpp` / `VideoUtil-v1.1.h`
   비디오 파일에서 프레임을 읽고, 디코딩하며, 필요한 전처리 작업을 수행하는 등의 비디오 처리 유틸리티 함수와 클래스를 제공합니다.
*** `HandTurnDetector.hpp`
   비디오 프레임 내에서 손의 움직임이나 방향 전환을 감지하는 데 사용되는 헬퍼 함수 또는 클래스를 정의합니다. 이는 비디오 전처리 단계의 일부일 수 있습니다.
*** `gRPCThread_.h`
   비동기 gRPC 통신이나 시간이 오래 걸리는 비디오 처리 작업을 백그라운드에서 수행하기 위한 스레드 관리 헬퍼 클래스 또는 함수를 포함합니다.

** 2. 진입점
*** `WinMain` (애플리케이션 진입점)
   `gRPCFileClientDlg.cpp`의 `#pragma comment(linker, "/entry:WinMainCRTStartup /subsystem:console")` 지시자를 통해 `WinMainCRTStartup`이 시작됩니다. 실질적인 MFC 애플리케이션의 초기화는 `gRPCFileClient.cpp`의 `CgRPCFileClientApp::InitInstance()` 함수에서 이루어지며, 여기서 메인 다이얼로그(`CgRPCFileClientDlg`)를 생성하고 실행합니다.
*** 다이얼로그 (UI 진입점)
   `CgRPCFileClientDlg`는 사용자 인터페이스의 주요 진입점이며, 사용자 입력(예: 파일 선택, 시작/정지 버튼 클릭)을 처리합니다.
*** 스레드 (비동기 처리 진입점)
   `gRPCThread_.h`에 정의된 스레드 관련 클래스/함수들은 백그라운드에서 비디오 처리나 gRPC 통신과 같은 시간이 오래 걸리는 작업을 비동기적으로 수행하는 진입점을 제공합니다.

** 3. 아키텍처 요약
*** UI 계층 (UI Layer)
   `gRPCFileClientDlg.cpp/.h`를 중심으로 MFC(Microsoft Foundation Classes)를 사용하여 사용자 인터페이스를 구성합니다. 사용자의 입력(예: 비디오 파일 선택, 분석 시작/중지)을 받고, gRPC 통신 결과를 화면에 표시하는 역할을 합니다. `framework.h`, `resource.h` 등이 이 계층을 지원합니다.
*** gRPC 통신 계층 (gRPC Communication Layer)
   `ksl_sentence_recognition.proto`에 정의된 프로토콜을 기반으로 서버와 통신합니다. `ksl_sentence_recognition*.pb.{h,cc}` 파일들은 프로토콜 버퍼 메시지와 gRPC 클라이언트 스텁을 제공하며, `gRPCFileClient.cpp/.h`의 핵심 로직이 이 스텁을 사용하여 서버에 요청을 보내고 응답을 처리합니다.
*** 비디오/전처리 계층 (Video/Preprocessing Layer)
   `VideoUtil-v1.1.cpp/.h`는 비디오 파일을 읽고 프레임을 추출하는 등의 비디오 처리 유틸리티를 제공합니다. `HandTurnDetector.hpp`는 비디오 프레임에서 특정 제스처나 특징을 감지하는 전처리 로직을 포함할 수 있습니다. 이 계층은 gRPC 서버로 전송될 데이터를 준비합니다.
*** ACR / AI 모듈 통합 (ACR / AI Module Integration)
   클라이언트 자체에 ACR(Automated Content Recognition) 또는 AI 모듈이 직접 포함되어 있지는 않습니다. 대신, 클라이언트는 gRPC 통신 계층을 통해 원격 서버에 위치한 ACR/AI 모듈에 비디오 데이터를 전송하고, 서버에서 처리된 결과를 받아오는 방식으로 통합됩니다. 즉, 클라이언트는 AI 서비스의 '소비자' 역할을 합니다.

** 4. 데이터 흐름도
1.  **사용자 액션 (User Action)**: 사용자가 `CgRPCFileClientDlg` UI에서 비디오 파일을 선택하고 "분석 시작" 버튼을 클릭합니다.
2.  **내부 로직 (Internal Logic)**:
    *   `CgRPCFileClientDlg`는 사용자 액션을 감지하고, `gRPCFileClient`의 관련 함수를 호출합니다.
    *   `gRPCFileClient`는 `VideoUtil-v1.1`을 사용하여 선택된 비디오 파일에서 프레임을 읽고, 필요에 따라 `HandTurnDetector`를 통해 전처리합니다.
    *   이 과정은 `gRPCThread_.h`에 정의된 스레드를 통해 백그라운드에서 비동기적으로 수행될 수 있습니다.
3.  **gRPC 요청 (gRPC Request)**:
    *   처리된 비디오 프레임 데이터는 `ksl_sentence_recognition.proto`에 정의된 메시지 형식으로 변환됩니다.
    *   `gRPCFileClient`는 생성된 gRPC 클라이언트 스텁(`ksl_sentence_recognition.grpc.pb.h`에 정의)을 사용하여 원격 gRPC 서버로 비디오 데이터와 함께 분석 요청을 보냅니다.
4.  **서버 응답 (Server Response)**:
    *   gRPC 서버는 요청을 받아 ACR/AI 모듈로 비디오 데이터를 전달하여 분석을 수행합니다.
    *   분석 결과(예: 인식된 수어 문장, 제스처 정보)는 다시 `ksl_sentence_recognition.proto`에 정의된 응답 메시지 형식으로 클라이언트에 반환됩니다.
5.  **UI 업데이트 (UI Update)**:
    *   `gRPCFileClient`는 서버로부터 받은 gRPC 응답을 처리합니다.
    *   처리된 결과는 `CgRPCFileClientDlg`로 전달되어, UI에 인식된 문장, 진행 상황 또는 기타 관련 정보를 표시하여 사용자에게 피드백을 제공합니다.
```

* 데이터 흐름 분석-1
코드베이스 분석이 완료되었습니다. 이제부터 요청하신 4가지 항목에 맞춰 시스템 아키텍처 분석 결과를 정리해 드리겠습니다.

*** 1. 전체 시스템 & 스레드 아키텍처 (Mermaid Diagram)
이 시스템은 4개의 주요 스레드가 MFC의 Event 객체를 통해 동기화하며 상호작용하는 구조입니다.

- *Main UI Thread*: 사용자의 입력을 받고, 화면을 갱신하며 다른 스레드를 생성하고 관리합니다.
- *gRPC Thread (=gRPCSend=)*: 주 작업 루프를 실행합니다. 비디오 파일을 읽고, 프레임을 추출한 뒤, AI Thread에 처리를 요청하고, 그 결과를 서버에 전송하는 전체 흐름을 관장합니다.
- *AI Thread (=AIThread=)*: gRPC Thread의 요청을 받아 =cv::Mat= 프레임을 입력받습니다. 내장된 Python 인터프리터를 통해 MediaPipe 같은 AI 모델을 실행하여 Skeleton Landmark(=vector<Point3f>=)를 추출하고 결과를 반환합니다.
- *Video Thread (=VideoPlay=)*: (사용자가 재생 버튼을 눌렀을 때) 별도로 비디오를 화면에 재생하는 역할을 합니다. 주 데이터 처리 흐름과는 독립적으로 동작합니다.

*상호작용 흐름:*

1. =gRPC Thread=가 비디오 프레임을 =CgRPCFileClientDlg::m_cap_img=에 준비합니다.
2. =gRPC Thread=가 =SetEvent(hAIStart)=를 호출하여 =AI Thread=를 깨웁니다.
3. =AI Thread=는 =m_cap_img=를 가져와 Python AI 모듈로 처리하고, 결과 Skeleton을 =m_sgcp_vtSkeletonMP=에 저장합니다.
4. =AI Thread=가 작업 완료 후 =SetEvent(hAIFinish)=를 호출합니다.
5. =gRPC Thread=는 =WaitForSingleObject(hAIFinish, ...)= 대기 상태에서 깨어나 =m_sgcp_vtSkeletonMP= 결과를 확인합니다.
6. 결과를 =Frame= protobuf 메시지로 인코딩하여 서버에 스트리밍 전송합니다.
7. 이 과정이 비디오의 지정된 범위 끝까지 반복됩니다.

#+begin_src mermaid
sequenceDiagram
    participant MainUI as Main UI Thread
    participant gRPC as gRPC Thread
    participant AI as AI Thread

    MainUI->>gRPC: Start gRPCSend Thread
    activate gRPC

    loop For each video frame
        gRPC->>gRPC: 1. Read frame from video file
        gRPC->>gRPC: 2. Pre-process frame (ROI, Resize)
        gRPC-->>AI: 3. SetEvent(hAIStart)

        activate AI
        AI->>AI: 4. Process frame (cv::Mat) with Python model
        AI->>AI: 5. Store skeleton (vector<Point3f>)
        AI-->>gRPC: 6. SetEvent(hAIFinish)
        deactivate AI

        gRPC->>gRPC: 7. Wait for hAIFinish
        gRPC->>gRPC: 8. Get skeleton data
        gRPC->>gRPC: 9. Encode frame and skeleton to Protobuf
        gRPC->>Server: 10. Send Frame message
    end

    gRPC-->>MainUI: Finish processing
    deactivate gRPC
#+end_src

*** 2. 데이터 라이프사이클 (Data Lifecycle Flow)
비디오 프레임 하나의 생성부터 소멸(전송)까지의 과정은 다음과 같이 5단계로 나눌 수 있습니다.

1. *캡처 (Capture):*
   - *위치*: =gRPCThread_.h=의 =SendFrames= 함수 내 =while= 루프
   - *변수*: =cv::Mat img=
   - *설명*: =cv::VideoCapture::read()= 또는 =cap >> img= 를 통해 비디오 파일로부터 원본 프레임(BGR, 3-channel)이 =cv::Mat= 형태로 메모리에 로드됩니다.
2. *전처리 (Pre-processing):*
   - *위치*: =gRPCThread_.h=의 =SendFrames= 함수 내 =while= 루프
   - *변수*: =cv::Mat roi=, =cv::Mat resize_roi_img=, =CgRPCFileClientDlg::m_cap_img=
   - *설명*: 원본 =img= 에서 사용자가 지정한 =m_roi=(Region of Interest) 영역만큼 =cv::Mat roi= 로 잘라냅니다. 이 =roi= 이미지는 성능을 위해 320px 너비 기준으로 리사이즈되어 다이얼로그의 멤버 변수인 =m_cap_img= 에 저장됩니다. 이 변수가 AI 스레드로 전달될 데이터입니다.
3. *AI 추론 (AI Inference):*
   - *위치*: =gRPCFileClientDlg.cpp= 의 =AIThread= 함수
   - *데이터 변환*: =cv::Mat= (C++) → =numpy.array= (Python Input) → =vector<Point3f>= (Python Output)
   - *설명*: =hAIStart= 이벤트로 =AIThread= 가 실행되면, =m_cap_img=를 =pybind11= type caster를 통해 Python의 =numpy.array=로 변환하여 AI 함수에 전달합니다. AI 함수는 MediaPipe를 이용해 자세를 추정하고, 그 결과인 3D 좌표 리스트를 =std::vector<cv::Point3f>= 형태로 반환받아 =m_sgcp_vtSkeletonMP= 멤버 변수에 저장합니다.
4. *후처리 및 분석 (Post-processing & Analysis):*
   - *위치*: =gRPCThread_.h= 의 =SendFrames= 함수
   - *변수*: =std::vector<cv::Point3f> m_sgcp_vtSkeletonMP=
   - *설명*: =hAIFinish= 이벤트 후, =gRPC Thread= 는 =m_sgcp_vtSkeletonMP=에 저장된 skeleton 데이터를 사용하여 =GetMotionStatusFromMP=, =HandTurnDetector::update= 등의 함수를 호출합니다. 이를 통해 손의 상태, 모션의 종류, 방향 전환 여부 등을 분석하여 프레임 전송 여부를 결정합니다.
5. *직렬화 및 전송 (Serialization & Transmission):*
   - *위치*: =gRPCThread_.h= 의 =EncodeFrame= 및 =SendFrames= 함수
   - *데이터 변환*: =cv::Mat=, =vector<Point3f>= → =vision::raw::v1::Frame= (Protobuf)
   - *설명*: 전송이 결정되면, =EncodeFrame= 함수가 호출됩니다. 이 함수는 2단계의 =roi= 이미지(=cv::Mat=)와 3단계의 =m_sgcp_vtSkeletonMP =(=vector<Point3f>)= 를 =ksl_sentence_recognition.proto= 에 정의된 =Frame= protobuf 메시지 형식으로 직렬화합니다. 이미지 데이터는 =bytes= 필드로, skeleton 좌표는 =repeated Point3= 필드로 변환되어 =writer->Write(frame)= 을 통해 서버로 전송됩니다.

*** 3. 핵심 함수 I/O 분석표 :ATTACH:
:PROPERTIES:
:ID:       cf4bfb85-ccba-4070-8b97-c6b2df8c00d1
:END:
| 함수명                 | 파일 위치               | 입력 (Input)                                                                                 | 출력 (Output)                                                   | 역할                                                                                                             |
|-----------------------+-----------------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------|
| AIThread              | gRPCFileClientDlg.cpp | LPVOID pParam (this 포인터). 내부적으로 m_cap_img (cv::Mat) 사용                                 | UINT (스레드 종료 코드). 내부적으로 m_sgcp_vtSkeletonMP (vector) 저장 | hAIStart 이벤트 수신 후 m_cap_img → Python AI 모듈 전달. skeleton 결과 → m_sgcp_vtSkeletonMP 저장. hAIFinish 이벤트 발생 |
| SendFrames            | gRPCThread_.h         | const std::string& session_id                                                               | void                                                           | 비디오 프레임 순차 읽기. AIThread 처리 요청. skeleton 데이터와 프레임 이미지 → gRPC 스트림 전송                               |
| GetMotionStatusFromMP | gRPCFileClientDlg.cpp | int cur_motion_status, vector(Point3f) cur_mp                                               | int (새로운 모션 상태)                                            | MediaPipe skeleton 분석. 손 위치 기반 상태 판단: 준비, 수어 발화, 급격 동작 등                                             |
| EncodeFrame           | gRPCThread_.h         | const cv::Mat& img, const string& session_id, int index, int flag, vector(Point3f) mpPose3D | Frame* out (출력 파라미터)                                        | cv::Mat 이미지와 skeleton 데이터 → Frame Protobuf 메시지 직렬화                                                       |
| DrawImageBMP          | VideoUtil-v1.1.h      | CWnd* wnd, Mat frame, int x, int y, ...                                                     | void                                                           | OpenCV Mat → MFC Picture Control 표시용 GDI BMP 변환                                                              |

#+ATTR_HTML: :width 1200px
[[attachment:_20251215_114155screenshot.png]]

*** 4. C++ <-> Python 데이터 변환 상세
C++과 Python 간의 데이터 변환은 =gRPCFileClientDlg.h=에 정의된 =pybind11::detail::type_caster<cv::Mat>= 커스텀 캐스터를 통해 *메모리 복사 없이(zero-copy)* 효율적으로 이루어집니다.

*1. C++ =cv::Mat= → Python =numpy.ndarray= (함수 호출 시)*

- *과정*: C++ 코드에서 Python 함수를 호출하며 =cv::Mat= 객체를 인자로 넘길 때 =type_caster=의 =cast= 함수가 실행됩니다.
- *동작 방식*:
  1. =cv::Mat= 객체의 내부 정보를 읽습니다: =rows=, =cols=, =channels=, =type=, 그리고 데이터 버퍼의 시작 주소(=mat.data=).
  2. 이 정보를 바탕으로 =numpy.ndarray=를 생성하기 위한 메타데이터(=buffer_info=)를 구성합니다. 이 메타데이터에는 데이터 타입, 차원(shape), 그리고 각 차원의 보폭(strides) 정보가 포함됩니다.
  3. 핵심은 =buffer_info=의 첫 번째 인자로 =mat.data=, 즉 C++ =cv::Mat=이 관리하는 *메모리 버퍼의 포인터*를 그대로 넘긴다는 점입니다.
  4. =pybind11::array=는 이 =buffer_info=를 받아 =numpy.ndarray=를 생성합니다. 이 =numpy= 배열은 데이터를 직접 소유하지 않고, C++의 =cv::Mat= 버퍼를 직접 참조(wrapping)하게 됩니다.
- *결과*: Python 함수는 C++ 메모리를 직접 읽고 처리하는 =numpy.ndarray=를 받게 되므로, 이미지 데이터 복사에 따른 오버헤드가 없습니다.

*2. Python =numpy.ndarray= → C++ =cv::Mat= (함수 반환 시)*

- *과정*: Python 함수가 =numpy.ndarray=를 반환하면, =type_caster=의 =load= 함수가 이 객체를 C++ =cv::Mat=으로 변환하기 위해 실행됩니다.
- *동작 방식*:
  1. Python으로부터 받은 =handle obj=를 =pybind11::array=로 캐스팅합니다.
  2. 배열의 =request()= 메소드를 호출하여 =buffer_info=를 요청합니다. 여기에는 =numpy= 배열의 차원, 데이터 타입, 데이터 시작 주소(=info.ptr=) 등의 정보가 담겨있습니다.
  3. =cv::Mat= 생성자 =cv::Mat(rows, cols, type, data_pointer)=를 호출하여 =cv::Mat= 객체를 생성합니다.
  4. 이때 데이터 포인터로 =info.ptr=, 즉 Python =numpy= 배열이 관리하는 *메모리 버퍼의 포인터*를 그대로 사용합니다.
- *결과*: C++ 코드는 Python 메모리를 직접 참조하는 =cv::Mat= 래퍼(wrapper) 객체를 받게 됩니다. 이 또한 메모리 복사가 발생하지 않습니다. 다만, 반환된 =cv::Mat=의 생명주기는 Python 객체의 생명주기에 의존하므로, 필요시 =.clone()=을 통해 데이터를 복사하여 소유권을 C++로 가져와야 합니다.

이러한 제로카피 방식은 고해상도 비디오 프레임을 실시간으로 처리해야 하는 이 시스템의 성능에 매우 중요합니다.

* 데이터 흐름 분석-2
=codebase_investigator= 를 통한 분석이 완료되었습니다. 결과를 바탕으로 프로젝트의 시스템 아키텍처를 요청하신 4가지 항목에 맞춰 상세히 정리해 드립니다.

*** 1. 전체 시스템 & 스레드 아키텍처 (Mermaid Diagram)
이 시스템은 MFC C++ 애플리케이션으로, UI, AI 처리, 네트워크 통신을 분리하기 위해 멀티 스레드 아키텍처를 사용합니다.

- *Main UI Thread (=CgRPCFileClientDlg=):* 사용자의 입력을 받아 gRPC Thread를 시작하고, Video Thread를 통해 화면을 갱신하는 메인 스레드입니다.
- *gRPC Thread (=gRPCSend=):* 핵심 워커 스레드입니다. 비디오 프레임을 읽고, AI Thread에 처리를 요청한 뒤, 그 결과를 받아 키 프레임 여부를 판단합니다. 키 프레임은 Protobuf 메시지로 인코딩되어 서버로 전송됩니다.
- *AI Thread (=AIThread=):* =gRPC Thread=로부터 =hAIStart= 이벤트를 받아 활성화됩니다. =cv::Mat= 이미지를 =pybind11=을 통해 Python 함수로 넘겨 자세 추정(Pose Estimation)을 수행하고, 결과인 스켈레톤 데이터를 C++로 반환한 뒤 =hAIFinish= 이벤트를 발생시켜 작업 완료를 알립니다.
- *Video Thread (=VideoPlay=):* UI에 비디오를 재생하는 역할만 독립적으로 수행합니다.

아래는 스레드 간의 상호작용을 나타낸 Mermaid 시퀀스 다이어그램입니다.

#+begin_src mermaid
sequenceDiagram
    participant MainUI as Main UI Thread
    participant gRPCThread as gRPC Thread
    participant AIThread as AI Thread
    participant Server as gRPC Server

    MainUI->>gRPCThread: 전송 시작 (OnBnClickedButton9)
    activate gRPCThread
    gRPCThread->>gRPCThread: SendFrames() 루프 시작
    loop 모든 비디오 프레임에 대해
        gRPCThread->>gRPCThread: 프레임 읽기 (cap >> img)
        gRPCThread->>AIThread: AI 처리 요청 (SetEvent(hAIStart))
        activate AIThread
        AIThread->>AIThread: Python 자세 추정 함수 호출
        AIThread-->>gRPCThread: AI 처리 완료 (SetEvent(hAIFinish))
        deactivate AIThread
        gRPCThread->>gRPCThread: AI 완료 대기 (WaitForSingleObject(hAIFinish))
        gRPCThread->>gRPCThread: 모션 분석 및 키 프레임 판단
        alt 키 프레임일 경우
            gRPCThread->>gRPCThread: 프레임 인코딩 (EncodeFrame)
            gRPCThread->>Server: Protobuf 메시지 전송 (writer->Write)
        end
    end
    gRPCThread-->>MainUI: 전송 완료
    deactivate gRPCThread
#+end_src

*** 2. 데이터 라이프사이클 (Data Lifecycle Flow)
비디오 프레임 하나가 생성되어 서버로 전송되기까지의 과정은 다음과 같은 5단계로 이루어집니다.

1. *생성 (C++):* =gRPCThread_.h=의 =SendFrames= 함수 내에서 =cv::VideoCapture=가 비디오 파일로부터 프레임 하나를 읽어 =cv::Mat img= 변수에 저장합니다. 이 이미지는 분석에 필요한 영역(ROI)만 잘라내어 =cv::Mat pDlg->m_cap_img=에 담깁니다.
2. *AI 분석 요청 (C++ -> Python):* =AIThread=에서 =pDlg->m_cap_img=(=cv::Mat=)가 =pybind11::cast=를 통해 Python의 =mediapipe_pose_func= 함수로 전달됩니다. 이 과정에서 =cv::Mat=은 =pybind11=에 의해 자동으로 *NumPy 배열*로 변환됩니다.
3. *AI 결과 반환 (Python -> C++):* Python 함수는 =[[x1, y1, z1], [x2, y2, z2], ...]= 형태의 *리스트(list of lists of doubles)*를 반환합니다. 이 값은 =.cast<std::vector<std::vector<double>>>()=를 통해 C++의 =std::vector<std::vector<double>>= 타입으로 변환됩니다.
4. *데이터 가공 (C++):* C++로 변환된 스켈레톤 데이터는 최종적으로 =std::vector<cv::Point3f>= 타입의 =pDlg->m_sgcp_vtSkeletonMP= 변수에 저장되어 모션 분석 및 인코딩에 사용됩니다.
5. *인코딩 및 전송 (C++ -> Protobuf):* =EncodeFrame= 함수가 원본 =cv::Mat= 이미지와 =vector<cv::Point3f>= 스켈레톤 데이터를 입력받아, =ksl_sentence_recognition.proto=에 정의된 *=Frame= Protobuf 메시지*로 직렬화(serialize)한 후 gRPC 스트림을 통해 서버로 전송합니다.

*** 3. 핵심 함수 I/O 분석표
주요 함수의 입력(Input), 출력(Output), 그리고 역할은 다음과 같습니다.

| 함수                    | 입력 (Input)                                                        | 출력 (Output)                                             | 역할                                                                                                                          |
|-------------------------+---------------------------------------------------------------------+-----------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------|
| *AIThread*              | =pDlg->m_cap_img= (cv::Mat), =hAIStart= (Event)                     | =pDlg->m_sgcp_vtSkeletonMP= (vector), =hAIFinish= (Event) | =hAIStart= 이벤트 수신 시, 입력된 이미지로 Python 자세 추정 모듈을 호출하고 스켈레톤 결과를 저장한 뒤 =hAIFinish= 이벤트 발생 |
| *SendFrames*            | =session_id= (string), 비디오 파일 경로, 프레임 범위                | (없음)                                                    | 프레임 읽기, AI 처리 조율, 모션 분석, 인코딩, gRPC 전송까지의 전체 데이터 파이프라인을 관리하는 메인 루프                     |
| *GetMotionStatusFromMP* | =cur_motion_status= (int), =cur_mp= (vector)                        | =motion_status= (int)                                     | 손의 위치 등 스켈레톤 데이터를 기반으로 '준비', '발화 중' 등 수어 상태를 판단하는 C++ 상태 머신                               |
| *EncodeFrame*           | =img= (cv::Mat), =session_id=, =index=, =flag=, =mpPose3D= (vector) | =out= (protobuf Frame*)                                   | =cv::Mat= 이미지와 스켈레톤 데이터를 네트워크 전송을 위한 Protobuf =Frame= 메시지로 직렬화                                    |
| *DrawImageBMP*          | =wnd= (CWnd*), =frame= (cv::Mat), 좌표                              | (없음)                                                    | =VideoUtil=에 포함된 유틸리티 함수로, =cv::Mat= 객체를 MFC UI의 특정 창에 렌더링                                              |

*** 4. C++ <-> Python 데이터 변환 상세
C++과 Python 간의 데이터 변환은 =gRPCFileClientDlg.cpp=의 =AIThread= 함수 내에서 =pybind11=을 통해 이루어집니다.

- *C++에서 Python으로 (=cv::Mat= -> =numpy.array=):* C++의 =cv::Mat= 객체(=dst_img=)가 Python 함수로 직접 전달됩니다. =pybind11=의 내장 캐스팅 기능이 =cv::Mat=을 Python에서 사용 가능한 NumPy 배열로 자동으로 변환해줍니다. (내부적으로 =pybind11_opencv=와 같은 바인딩 라이브러리가 필요할 수 있습니다.)

  #+begin_src cpp
  // gRPCFileClientDlg.cpp 내 AIThread 함수

  // pybind11::cast가 cv::Mat을 NumPy 배열로 자동 변환하여 func_mp_pose 함수에 전달
  auto result_pose_mp = func_mp_pose(pybind11::cast(dst_img));
  #+end_src

- *Python에서 C++로 (=list= -> =std::vector=):* Python 함수가 반환한 Python 객체(중첩 리스트)는 =.cast<T>()= 메소드를 통해 명시된 C++ 타입으로 변환됩니다.

  #+begin_src cpp
  // gRPCFileClientDlg.cpp 내 AIThread 함수

  // Python의 반환값(list of lists)을 C++의 vector<vector<double>>로 변환
  auto mp_result = result_pose_mp.cast<std::vector<std::vector<double>>>();
  #+end_src

* 데이터 흐름 분서-3
분석이 완료되었습니다. =gRPCKSLDetectorClient=는 *MFC 기반의 멀티스레드 아키텍처*를 가지며, *Python(MediaPipe)과 C++(Heuristic Logic)의 하이브리드 방식*으로 동작합니다. 요청하신 4가지 항목에 대한 상세 분석 결과입니다.

*** 1. 전체 시스템 & 스레드 아키텍처 (Mermaid Diagram) :ATTACH:
:PROPERTIES:
:ID:       ea8432d1-3f72-4394-895c-54974c912d53
:END:
이 시스템은 *Event(=hAIStart=, =hAIFinish=)* 를 통해 =gRPC Thread=와 =AI Thread=가 강하게 동기화(Synchronous)되어 동작하는 구조입니다.

[[attachment:_20251215_122418screenshot.png]]

#+begin_src mermaid
sequenceDiagram
    autonumber
    participant gRPC as gRPC Thread<br/>(Function: SendFrames)
    participant AI as AI Thread<br/>(Function: AIThread)
    participant Python as Python Engine<br/>(Module: ACR.mp_detect)

    Note over gRPC: 1. Creation (Raw Input)\nVar: img, roi\nType: cv::Mat
    gRPC->>gRPC: Read frame from video file into img\nCrop img to ROI

    Note over gRPC: 2. Shared Memory Write\nVar: m_cap_img\nType: cv::Mat
    gRPC->>gRPC: Resize ROI and assign to m_cap_img

    gRPC->>AI: SetEvent(hAIStart)\n(Trigger AI Processing)
    activate AI

    Note right of gRPC: gRPC Thread Blocked\nWaiting for hAIFinish...

    Note over AI: 3. Python Conversion (Input)\nVar: (internal)\nType: numpy.ndarray
    AI->>Python: Call func_mp_pose with m_cap_img\n(Implicit pybind11 cast)
    activate Python

    Note over Python: Inference (MediaPipe)

    Python-->>AI: Return Result

    deactivate Python

    Note over AI: 4. Python Conversion (Output)\nVar: mp_result\nType: vector<vector<double>>\nVar: m_sgcp_vtSkeletonMP\nType: vector<Point3f>
    AI->>AI: Cast Python result to mp_result\nReconstruct to m_sgcp_vtSkeletonMP

    AI-->>gRPC: SetEvent(hAIFinish)\n(Signal Completion)
    deactivate AI

    gRPC->>gRPC: Wake Up (Wait Finished)

    Note over gRPC: 5. Logic & Serialization\nVar: frame\nType: vision::raw::v1::Frame
    gRPC->>gRPC: EncodeFrame(roi, m_sgcp_vtSkeletonMP)\nSerialize to Protobuf Message

    gRPC->>gRPC: writer->Write(frame)\n(Stream to Server)

#+end_src

- *동기화 포인트:* =gRPC Thread=는 AI 처리가 끝날 때까지 멈춰 있습니다(=WaitForSingleObject=). 즉, 전체 프레임 처리 속도는 Python AI 모델의 추론 속도에 종속됩니다.

*** 2. 데이터 라이프사이클 (Data Lifecycle Flow)
비디오 프레임이 생성되어 전송되기까지의 5단계 변환 과정입니다.

1. *Creation (Raw Input):*
   - =gRPCThread_.h= :: =SendFrames=
   - =cv::VideoCapture=에서 =cv::Mat img=로 원본 프레임을 읽습니다.
   - 설정된 ROI(관심 영역)로 자르고(=img(m_roi)=), 필요시 320px 너비로 리사이즈하여 * =cv::Mat m_cap_img= * (멤버 변수)에 저장합니다.
2. *Bridge (C++ to Python):*
   - =gRPCFileClientDlg.cpp= :: =AIThread=
   - =m_cap_img=가 =pybind11=의 =type_caster=를 통해 * =numpy.ndarray= * 로 변환되어 Python 함수 =func_mp_pose=로 전달됩니다. (Data Copy 발생 가능성 있음, type_caster 구현에 따라 다름)
3. *Inference (Python Logic):*
   - Python (=ACR.mp_detect=) 내부에서 MediaPipe를 수행하고, 결과(Skeleton Joints)를 반환합니다.
   - 반환값은 =list of lists= 형태(=[[x...], [y...], [z...]]=)로 추정되며, C++로 돌아오면서 * =std::vector<std::vector<double>>= * 로 캐스팅됩니다.
4. *Logic & Serialization (C++):*
   - =gRPCFileClientDlg.cpp= :: =AIThread=
   - Python 결과가 *=std::vector<cv::Point3f> m_sgcp_vtSkeletonMP=* 로 변환되어 저장됩니다.
   - =gRPCThread_.h= 에서 이 좌표 데이터를 =HandTurnDetector= (C++ 클래스)에 입력하여 동작(Turn) 여부를 판별합니다.
5. *Transmission (Network):*
   - =gRPCThread_.h= :: =EncodeFrame=
   - ROI 이미지(=cv::Mat=)와 골격 데이터(=vector<Point3f>=)가 * =ksl_sentence_recognition::Frame=  (Protobuf Message)* 로 직렬화됩니다.
   - 최종적으로 gRPC =writer->Write(frame)= 을 통해 서버로 스트리밍됩니다.

*** 3. 핵심 함수 I/O 분석표 (Table)
| 함수명                    | 파일 위치             | Input (Parameters)                               | Output (Return)    | 핵심 역할                                                                                              |
|---------------------------+-----------------------+--------------------------------------------------+--------------------+--------------------------------------------------------------------------------------------------------|
| *=AIThread=*              | gRPCFileClientDlg.cpp | =LPVOID pParam= (Dialog 포인터)                  | =UINT= (0)         | =hAIStart= 이벤트를 대기하다가, =m_cap_img=를 Python에 전달하고 결과를 =m_sgcp_vtSkeletonMP=에 저장    |
| *=SendFrames=*            | gRPCThread_.h         | =string session_id=                              | =void=             | 비디오 루프를 돌며 이미지 캡처, AI 스레드 트리거, C++ 로직 수행, gRPC 전송을 총괄                      |
| *=GetMotionStatusFromMP=* | gRPCFileClientDlg.cpp | =int cur_status=, =vector<Point3f> mp=           | =int= (New Status) | 이전 프레임의 골격 좌표와 비교하여 정지/말하기/급격동작 상태를 천이(State Machine)                     |
| *=EncodeFrame=*           | gRPCThread_.h         | =Mat img=, =vector<Point3f> mp=, =Frame* out= 등 | =bool=             | OpenCV 이미지와 3D 좌표를 Protobuf 메시지 포맷(=vision::raw::v1::Frame=)으로 변환(직렬화)              |
| *=DrawImageBMP=*          | VideoUtil             | =CWnd* pWnd=, =Mat img= 등                       | =void=             | OpenCV의 =Mat= 데이터를 Windows MFC 컨트롤(Picture Control)에 그리기 위해 GDI 비트맵으로 변환하여 출력 |

*** 4. C++ <-> Python 데이터 변환 상세 (pybind11)
이 프로젝트는 =pybind11=을 사용하여 C++와 Python 간의 데이터 변환을 *명시적인 Type Caster*를 통해 처리하고 있습니다. (=gRPCFileClientDlg.h=에 정의됨)

*1. =cv::Mat= → =numpy.ndarray= (C++ to Python)* * *코드 위치:* =pybind11::detail::type_caster<cv::Mat>::cast= * *동작:* =cv::Mat=의 메타데이터(rows, cols, channels)를 =pybind11::buffer_info= 구조체에 매핑하여 Numpy 배열을 생성합니다. * *특이사항:* =mat.data= 포인터를 공유하여 *Zero-Copy*에 가깝게 전달하려 하지만, Python 측에서 수명 관리를 위해 복사가 일어날 수 있습니다. 지원 타입은 =uchar=, =int32=, =float=로 제한되어 있습니다.

*2. Python List → =std::vector<cv::Point3f>= (Python to C++)* * *코드 위치:* =AIThread= 내부 * *변환 과정:* ```cpp // 1. Python 리턴값을 임시 객체로 받음 auto result_pose_mp = func_mp_pose(pybind11::cast(dst_img));

#+begin_example
// 2. Python List -> C++ Vector<Vector<double>> (자동 캐스팅)
auto mp_result = result_pose_mp.cast<std::vector<std::vector<double>>>();

// 3. Vector<Vector<double>> -> Vector<Point3f> (수동 변환)
// Python 반환값이 [ [x1, x2...], [y1, y2...], [z1, z2...] ] (SoA 구조)라고 가정됨
pDlg->m_sgcp_vtSkeletonMP.push_back(
    Point3f(mp_result[0][k], mp_result[1][k], mp_result[2][k])
);
```
#+end_example

- *데이터 구조:* Python 함수는 =N x 3= (Point list)가 아니라 *=3 x N= (Dimension list)* 형태의 데이터를 반환하는 것으로 보입니다. 이를 C++ 루프에서 =Point3f= 구조체 배열(=AoS=)로 재조립합니다.

* AIThread
** gemini-web
`AIThread`는 메인 로직(비디오 재생 또는 gRPC 전송 스레드)으로부터 영상 데이터가 준비되었다는 신호를 받으면, 내장된 Python 인터프리터를 통해 MediaPipe 로직을 수행하고 결과를 C++ 구조체에 저장합니다.
**** 1. 실행 대기 및 시작 신호 수신 (Wait for Signal)
   AI 스레드는 무한 루프 내에서 `hAIStart` 이벤트가 발생하기를 대기합니다. 이 이벤트는 메인 스레드나 비디오 스레드에서 `SetEvent(hAIStart)`를 호출할 때 활성화됩니다.
   - `WaitForSingleObject`: CPU 자원을 소모하지 않고 대기 상태로 머뭅니다.
   - `pDlg->m_cap_img`: 분석할 이미지가 비어있는지 안전 장치로 확인합니다.

   #+BEGIN_SRC cpp
   // gRPCFileClientDlg.cpp : AIThread 내부 Loop

   while (pDlg->m_exit.load() == 0)
   {
       // hAIStart 이벤트가 SetEvent 될 때까지 대기
       WaitForSingleObject(pDlg->hAIStart, INFINITE);

       // 영상 데이터가 비어있지 않은 경우에만 로직 수행
       if (!pDlg->m_cap_img.empty())
       {
           // ... (이후 로직 진행)
       }
       // ...
   }
   #+END_SRC

**** 2. 이미지 전처리 (Preprocessing)
   원본 이미지를 복제하고, Python 라이브러리(MediaPipe)가 처리하기 적합한 포맷으로 변환합니다.
   - `clone()`: 원본 `m_cap_img`가 다른 스레드(비디오 재생 등)에서 동시에 변경될 수 있으므로, 반드시 깊은 복사를 수행하여 스레드 안전성을 확보합니다.
   - `cvtColor`: OpenCV의 기본인 4채널(BGRA) 이미지인 경우 3채널(BGR)로 변환합니다.

   #+BEGIN_SRC cpp
   // 이미지 깊은 복사
   cv::Mat dst_img = pDlg->m_cap_img.clone();

   // 4채널(투명도 포함)인 경우 3채널로 변환
   if (dst_img.channels() == 4)
       cvtColor(dst_img, dst_img, COLOR_BGRA2BGR);
   #+END_SRC

**** 3. Python 함수 호출 및 추론 (Python Inference)
   `pybind11`을 사용하여 C++의 `cv::Mat`을 Python 객체로 변환하고, 스레드 초기화 시점에 로드해둔 Python 함수(`func_mp_pose`)를 호출합니다.
   - `pybind11::cast(dst_img)`: OpenCV Mat 데이터를 Python `numpy.ndarray`로 자동 변환하여 인자로 전달합니다.
   - `func_mp_pose(...)`: Python 영역(`ACR.mp_detect` 모듈)의 MediaPipe Pose 추론을 실행하고 결과를 반환받습니다.

   #+BEGIN_SRC cpp
   // Python 함수 호출 (이미지 전달 및 추론 실행)
   auto result_pose_mp = func_mp_pose(pybind11::cast(dst_img));
   #+END_SRC

**** 4. 결과 데이터 파싱 (Result Parsing)
   Python 리턴값을 C++ 데이터 타입으로 변환하고, 결과를 저장할 멤버 변수 벡터를 초기화합니다.
   - 반환값 구조: Python List `[[x1, x2...], [y1, y2...], [z1, z2...]]` 형태.
   - `cast<...>`: Python 객체를 C++의 `std::vector<std::vector<double>>` 타입으로 캐스팅합니다.

   #+BEGIN_SRC cpp
   // Python 반환값을 C++ 이중 벡터로 변환
   auto mp_result = result_pose_mp.cast<std::vector<std::vector <double>>>();

   // 기존에 저장된 스켈레톤 데이터 초기화
   pDlg->m_sgcp_vtSkeletonMP.clear();
   #+END_SRC

**** 5. 관절 좌표 저장 (Store Skeleton Data)
   파싱된 좌표 데이터를 순회하며 `Point3f` 객체로 생성하여 최종 저장합니다.
   - `mp_result[0]`: X 좌표 리스트
   - `mp_result[1]`: Y 좌표 리스트
   - `mp_result[2]`: Z 좌표 리스트
   - 위 3개 리스트의 같은 인덱스(`k`)를 조합하여 하나의 3D 좌표(`Point3f`)를 만듭니다.

   #+BEGIN_SRC cpp
   // 데이터 유효성 검사 (X, Y, Z 축 데이터가 모두 있는지)
   if (mp_result.size() >= 3)
   {
       // 랜드마크 개수만큼 반복
       for (int k = 0; k < mp_result[0].size(); k++)
       {
           // x, y, z 좌표를 묶어 Point3f로 저장
           pDlg->m_sgcp_vtSkeletonMP.push_back(
               Point3f(mp_result[0][k], mp_result[1][k], mp_result[2][k])
           );
       }
   }
   #+END_SRC

**** 6. 완료 신호 전송 (Signal Completion)
   모든 처리가 완료되었음을 알리는 이벤트를 발생시킵니다. 이를 통해 `WaitForSingleObject(hAIFinish)`로 대기 중이던 메인 로직(전송 스레드 등)이 깨어나 다음 단계(gRPC 전송 등)를 진행하게 됩니다.

   #+BEGIN_SRC cpp
   // 처리 완료 신호 발생
   SetEvent(pDlg->hAIFinish);
   #+END_SRC
** gemini-cli(함수 주석)
`AIThread`의 실행 과정을 코드와 함께 단계별로 설명하겠습니다. `AIThread`는 `gRPCThread_.h`에 정의된 `CWinThread`를 상속받는 클래스로 보이며, 실제 로직은 `gRPCFileClient.cpp`와 `gRPCFileClientDlg.cpp`에서 호출되고 실행됩니다.

```org
#+TITLE: AIThread 영상 처리 및 gRPC 통신 과정 분석

*** 1. 스레드 시작 및 초기화 (사용자 입력)
사용자가 UI에서 "Start" 버튼을 클릭하면 `CgRPCFileClientDlg::OnBnClickedButtonStart()` 함수가 호출됩니다. 이 함수는 gRPC 채널을 설정하고, `gRPCFileClient` 객체를 초기화한 뒤, `AIProcessingThread` (실제 `AIThread` 인스턴스)를 생성하고 시작합니다.

#+BEGIN_SRC cpp
// gRPCFileClientDlg.cpp
void CgRPCFileClientDlg::OnBnClickedButtonStart()
{
    // ... UI 컨트롤 비활성화 ...

    // gRPC 채널 생성 및 클라이언트 초기화
    std::string server_address("localhost:50051");
    stub_ = KSLSentenceRecognition::NewStub(grpc::CreateChannel(server_address, grpc::InsecureChannelCredentials()));
    g_gRPCClient = new gRPCFileClient(stub_.get());
    g_gRPCClient->set_dlg(this);

    // AI 처리 스레드 생성 및 시작
    // AfxBeginThread는 CWinThread를 상속받은 클래스의 인스턴스를 생성하고 스레드를 시작합니다.
    // AIProcessingThread는 CWinThread를 상속받은 클래스일 것입니다.
    p_th_ai_proc = (AIProcessingThread*)AfxBeginThread(
        RUNTIME_CLASS(AIProcessingThread), // 스레드 클래스
        THREAD_PRIORITY_NORMAL,
        0,
        CREATE_SUSPENDED); // 일단 정지 상태로 생성

    // 스레드에 필요한 정보(gRPC 클라이언트, 파일 경로 등)를 전달
    p_th_ai_proc->set_grpc_client(g_gRPCClient);
    p_th_ai_proc->set_video_path(std::string(CT2CA(str_video_path)));
    p_th_ai_proc->set_dlg(this);

    // 스레드 실행 재개
    p_th_ai_proc->ResumeThread();
}
#+END_SRC

*** 2. 스레드 메인 루프 진입 (`AIProcessingThread::InitInstance`)
`AfxBeginThread`에 의해 스레드가 시작되면, `CWinThread`의 가상 함수인 `InitInstance()`가 가장 먼저 호출됩니다. 이 함수는 스레드의 주된 로직을 포함하는 진입점 역할을 합니다.

#+BEGIN_SRC cpp
// gRPCThread_.h 또는 관련 구현 파일에 있을 것으로 예상되는 코드
BOOL AIProcessingThread::InitInstance()
{
    // 스레드의 메인 로직 함수인 Run()을 호출합니다.
    // 이 함수가 반환될 때까지 스레드는 살아있습니다.
    Run();
    return TRUE; // TRUE를 반환하여 스레드가 성공적으로 초기화되었음을 알림
}
#+END_SRC

*** 3. 비디오 처리 및 gRPC 스트리밍 (`AIProcessingThread::Run`)
`Run()` 함수는 스레드의 핵심 로직을 수행합니다. 비디오 파일을 열고, 프레임 단위로 읽어서 gRPC 서버로 스트리밍 전송을 시작합니다.

#+BEGIN_SRC cpp
// gRPCThread_.h 또는 관련 구현 파일에 있을 것으로 예상되는 코드
void AIProcessingThread::Run()
{
    // 1. 비디오 캡처 객체 생성 및 파일 열기
    cv::VideoCapture cap(video_path);
    if (!cap.isOpened()) {
        // ... 오류 처리 ...
        return;
    }

    // 2. gRPC 스트리밍 시작
    // gRPCFileClient의 StreamSentenceAnalyze 함수를 호출하여 서버와의 양방향 스트리밍을 시작합니다.
    // 이 함수는 내부적으로 gRPC 컨텍스트를 만들고, 서버로 데이터를 보내고 응답을 읽는 루프를 시작합니다.
    grpc_client->StreamSentenceAnalyze();

    // ... 프레임 처리 루프 ...
}
#+END_SRC

*** 4. 프레임 단위 데이터 전송 (`AIProcessingThread::Run` 루프 내부)
`Run()` 함수 내의 루프에서 `cv::VideoCapture`를 통해 비디오 프레임을 하나씩 읽습니다. 각 프레임은 `HandTurnDetector`로 전처리된 후, gRPC 메시지 형식(`SentenceRequest`)으로 변환되어 서버로 전송됩니다.

#+BEGIN_SRC cpp
// gRPCThread_.h 또는 관련 구현 파일에 있을 것으로 예상되는 코드 (Run 함수 내부)
void AIProcessingThread::Run()
{
    // ... (이전 단계 코드) ...

    int frame_count = 0;
    cv::Mat frame;
    HandTurnDetector hand_turn_detector; // 손 방향 감지기 초기화

    while (true)
    {
        // 4-1. 비디오에서 프레임 하나를 읽음
        cap >> frame;
        if (frame.empty()) {
            // 프레임이 없으면 비디오의 끝. 루프 종료.
            break;
        }

        // 4-2. 손 방향 감지 (전처리)
        // is_hand_turn_frame은 프레임에서 손의 방향 전환이 감지되었는지 여부를 나타냅니다.
        bool is_hand_turn_frame = hand_turn_detector.is_hand_turn_frame(frame);

        // 4-3. gRPC 요청 메시지 생성
        // 프레임 데이터와 메타데이터를 SentenceRequest 객체에 채웁니다.
        Image frame_img;
        frame_img.set_width(frame.cols);
        frame_img.set_height(frame.rows);
        frame_img.set_channel(frame.channels());
        frame_img.set_data(frame.data, frame.total() * frame.elemSize());

        SentenceRequest request;
        request.set_allocated_frame_img(&frame_img);
        request.set_frame_count(frame_count++);
        request.set_is_hand_turn_frame(is_hand_turn_frame);

        // 4-4. gRPC 스트림을 통해 서버로 요청 전송
        // gRPCFileClient의 SendSentenceRequest 함수를 호출하여 서버로 메시지를 보냅니다.
        grpc_client->SendSentenceRequest(request);

        // 중요: request 객체에서 frame_img의 소유권을 다시 가져와야 메모리 누수를 방지할 수 있습니다.
        request.release_frame_img();
    }

    // 5. 스트리밍 종료 신호 전송
    // 비디오의 모든 프레임을 보낸 후, 스트림의 끝을 알리는 신호를 서버에 보냅니다.
    grpc_client->WritesDone();
}
#+END_SRC

*** 5. 서버 응답 수신 및 UI 업데이트 (`gRPCFileClient::StreamSentenceAnalyze`의 읽기 스레드)
`gRPCFileClient::StreamSentenceAnalyze` 함수는 요청을 보내는 동시에, 별도의 읽기 스레드를 생성하여 서버로부터의 응답을 비동기적으로 기다립니다. 서버로부터 응답(`SentenceResponse`)이 오면, 이 데이터를 파싱하여 UI를 업데이트하도록 메인 다이얼로그에 메시지를 보냅니다.

#+BEGIN_SRC cpp
// gRPCFileClient.cpp
void gRPCFileClient::StreamSentenceAnalyze()
{
    // ... (컨텍스트 및 스트림 객체 초기화) ...

    // 읽기 스레드 생성: 서버로부터의 응답을 비동기적으로 처리
    std::thread read_thread([this]() {
        SentenceResponse response;
        // stream->Read()는 서버로부터 메시지가 올 때까지 블로킹됩니다.
        // 스트림이 닫힐 때까지 계속해서 응답을 읽습니다.
        while (stream->Read(&response)) {
            // 응답 처리
            std::string sentence = response.sentence();
            int type = response.type();

            // UI 스레드로 메시지를 보내 화면을 업데이트합니다.
            // PostMessage는 비동기적으로 메시지를 보내므로 UI 스레드의 작업을 방해하지 않습니다.
            // 사용자 정의 메시지인 WM_UPDATE_SENTENCE를 사용합니다.
            if (dlg) {
                // 응답 데이터를 동적으로 할당하여 메시지와 함께 전달
                ResponseData* pData = new ResponseData{ CString(sentence.c_str()), type };
                dlg->PostMessage(WM_UPDATE_SENTENCE, (WPARAM)pData, 0);
            }
        }
    });

    // 메인 스레드(AIProcessingThread)는 이 함수 호출 후 바로 리턴되지 않고,
    // 읽기 스레드가 종료될 때까지 기다립니다.
    read_thread.join();
}
#+END_SRC

*** 6. 스레드 종료
`AIProcessingThread::Run()` 함수의 루프가 모두 끝나고 `grpc_client->WritesDone()`이 호출되면, `gRPCFileClient`의 읽기 스레드도 서버로부터 스트림 종료 신호를 받고 종료됩니다. 결과적으로 `AIProcessingThread::Run()` 함수가 반환되고, `InitInstance()`도 종료되면서 `AIThread`는 모든 작업을 마치고 소멸합니다.
```
* 손동작 인식
** gemini-cli
*** 개요
MediaPipe Hands는 모바일, 데스크톱, 웹 등 다양한 플랫폼에서 실시간으로 손과 손가락을 추적하는 고성능 솔루션입니다. 핵심 기능은 한 손에 대해 **21개의 3D 랜드마크(Landmark)**를 감지하는 것입니다.

알고리즘의 핵심 아이디어는 **"먼저 넓은 영역에서 손바닥을 찾고, 그 다음에 찾은 영역 내에서 정밀하게 랜드마크를 찾는다"**는 2단계 접근 방식을 사용하여 정확도와 속도를 모두 확보하는 것입니다.
*** 핵심 알고리즘: 2단계 파이프라인 (Two-Stage Pipeline)

**** 1단계: 손바닥 감지 모델 (Palm Detection Model)
   이 단계의 목표는 전체 이미지에서 손이 어디에 있는지를 빠르고 가볍게 찾아내는 것입니다.

   - **역할**: 이미지 전체에서 손바닥 영역을 찾아 사각 영역(Bounding Box)으로 위치를 특정합니다.
   - **입력**: 전체 비디오 프레임 (예: 640x480 이미지)
   - **사용 모델**: BlazePalm 이라는 가벼운 단일 샷(Single-Shot) 감지 모델을 사용합니다. 이 모델은 모바일 GPU에서도 실시간으로 동작하도록 최적화되어 있습니다.
   - **출력**: 손바닥을 감싸는 회전된 사각 영역(Rotated Bounding Box)과 감지 신뢰도(Confidence Score).
   - **특징**:
     - 주먹을 쥔 손, 활짝 편 손, 장갑을 낀 손 등 다양한 형태의 손을 감지할 수 있습니다.
     - 손가락까지 모두 포함하는 대신, 손바닥 영역에 집중하여 모델 크기를 줄이고 속도를 높였습니다.

#+BEGIN_SRC text
+--------------------------------------+
|                                      |
|      +-------+                       |
|      |       |  <-- 1. BlazePalm이   |
|      | Palm  |      손바닥 영역 감지 |
|      | BBox  |                       |
|      +-------+                       |
|                                      |
+--------------------------------------+
        전체 이미지 프레임
#+END_SRC

**** 2단계: 손 랜드마크 모델 (Hand Landmark Model)
   1단계에서 찾은 손바닥 영역 내에서 21개의 정밀한 관절 위치를 찾아냅니다.

   - **역할**: 감지된 손바닥 영역 이미지 내에서 21개의 3D 랜드마크 좌표를 추정합니다.
   - **입력**: 1단계에서 찾은 손바닥 영역을 잘라내고, 손목 방향에 맞춰 회전 및 정규화한 작은 이미지.
   - **사용 모델**: 1단계 모델보다 더 무겁고 정밀한 리그레션(Regression) 기반 모델을 사용합니다.
   - **출력**:
     1. **21개의 3D 랜드마크 좌표 (x, y, z)**:
        - `x`, `y`: 이미지 내의 정규화된 좌표 (0.0 ~ 1.0).
        - `z`: 손목(Wrist, 랜드마크 0번)을 기준으로 한 상대적인 깊이. `z`값이 작을수록 카메라에 가깝습니다.
     2. **왼손/오른손 판별 (Handedness)**: 감지된 손이 왼손인지 오른손인지에 대한 신뢰도.
     3. **랜드마크 감지 신뢰도 (Confidence Score)**.

#+BEGIN_SRC text
+-------+
| .   . |
|  . .  |  <-- 2. 랜드마크 모델이
| .   . |      21개 관절 위치 추정
|  . .  |
+-------+
 잘라낸 손바닥 이미지
#+END_SRC

*** 알고리즘의 연속성 및 최적화 (Tracking & Optimization)
비디오처럼 연속된 프레임에서는 매번 1단계(손바닥 감지)를 실행하지 않고, 다음과 같은 최적화 과정을 거칩니다.

1.  **첫 프레임**: 1단계(Palm Detection)를 실행하여 손의 위치를 찾고, 그 결과로 2단계(Landmark Model)를 실행하여 21개 랜드마크를 찾습니다.
2.  **다음 프레임부터**: 이전 프레임에서 찾은 **랜드마크들을 기반으로** 현재 프레임의 손바닥 위치를 추정합니다. 그리고 바로 2단계(Landmark Model)를 실행합니다.
3.  **추적 실패 시**: 만약 2단계에서 랜드마크를 찾는 데 실패하거나 신뢰도가 특정 임계값 아래로 떨어지면 (예: 손이 너무 빨리 움직이거나 화면 밖으로 나갔을 때), 알고리즘은 추적에 실패했다고 판단하고 다시 1단계(Palm Detection)를 전체 프레임에 대해 실행하여 손의 위치를 찾습니다.

이러한 방식 덕분에 무거운 손바닥 감지 모델을 매 프레임마다 실행할 필요가 없어져서 계산 비용이 크게 줄어들고, 실시간 추적이 가능해집니다.

*** 3단계: 랜드마크를 제스처로 해석하기 (From Landmarks to Gestures)
MediaPipe는 21개 랜드마크의 '위치'를 제공할 뿐, "이것이 '주먹'이다" 또는 "'V' 사인이다"라고 직접 알려주지는 않습니다. 랜드마크 좌표를 실제 제스처로 해석하는 로직은 개발자가 직접 구현해야 합니다.

**** 방법 1: 기하학적 규칙 기반 (Rule-Based Geometric Approach)
   랜드마크 간의 각도나 거리를 계산하여 규칙을 만듭니다.

   - **예시: "검지 손가락이 펴졌는가?"**
     - 손목(0), 검지 시작(5), 검지 중간(6), 검지 끝(8) 랜드마크를 이용합니다.
     - 벡터 (5 -> 6)과 벡터 (6 -> 8) 사이의 각도를 계산합니다.
     - 이 각도가 160~180도에 가까우면 "펴진 상태"로 판단할 수 있습니다.
     - `is_finger_straight = angle(vec(5,6), vec(6,8)) > 160`

   - **예시: "주먹을 쥐었는가?"**
     - 모든 손가락 끝(4, 8, 12, 16, 20)이 손바닥 중심(예: 0번 랜드마크)과 특정 거리 이하로 가까워졌는지 확인합니다.

**** 방법 2: 머신러닝 모델 기반 (Machine Learning Approach)
   더 복잡하고 다양한 제스처를 인식하기 위해, 랜드마크 데이터를 입력으로 하는 별도의 분류(Classification) 모델을 학습시킵니다.

   1.  **데이터 수집**: '주먹', 'V 사인', 'OK' 등 인식하고자 하는 각 제스처에 대한 랜드마크 데이터를 수집하고 라벨링합니다.
   2.  **특징 추출 (Feature Engineering)**:
       - 21개 랜드마크의 (x, y, z) 좌표를 그대로 사용하거나,
       - 손목(0번)을 원점으로 하여 모든 좌표를 상대적으로 변환하고,
       - 손 크기에 대해 정규화(Normalization)하여 특징 벡터(Feature Vector)를 만듭니다.
   3.  **모델 학습**: 준비된 특징 벡터와 라벨을 사용하여 SVM(Support Vector Machine), RandomForest, 또는 간단한 신경망(MLP) 같은 분류 모델을 학습시킵니다.
   4.  **추론**: 실시간으로 MediaPipe에서 얻은 랜드마크 데이터로 특징 벡터를 만들어 학습된 모델에 입력하면, 어떤 제스처인지 분류된 결과가 나옵니다.
**
** chatgpt
*** MediaPipe 기반 손동작 인식 전체 흐름

**** 1. 입력 영상과 ROI 준비 단계

*** OpenCV `VideoCapture` 로 CCTV/영상 프레임 읽기
*** 전체 프레임에서 사용자 지정 ROI(rect) 잘라서 `m_cap_img` 에 저장
*** `m_cap_img` 변수

  * AI 스레드와 공유되는 현재 프레임의 관심 영역 이미지 버퍼 역할

**** 2. AIThread 스레드와 pybind11 초기화

*** `CgRPCFileClientDlg::AIThread(LPVOID pParam)`

  * 별도 CWinThread 로 동작하는 AI 전용 스레드
  * 메인 다이얼로그 포인터 `pDlg` 사용
*** 동기화 이벤트

  * `HANDLE hAIStart`

    * 메인 쪽에서 AI 분석 시작 신호 전달용 이벤트
  * `HANDLE hAIFinish`

    * AIThread 가 분석 완료를 알리는 이벤트
*** Python 임베딩 초기화

  * `pybind11::scoped_interpreter guard{}` 로 내장 Python 인터프리터 초기화
  * AIThread 수명 동안 Python 런타임 유지

**** 3. Python MediaPipe 함수 호출 단계

*** Python 모듈 로딩

  * `auto exampleModule4 = pybind11::module_::import("ACR.mp_detect");`
  * C++ 에서 `ACR/mp_detect.py` 모듈 직접 임포트
*** MediaPipe 관련 함수 포인터 획득

  * `auto func_mp_pose = exampleModule4.attr("mediapipe_pose_func");`
  * `auto func_mp_hand = exampleModule4.attr("mediapipe_hand_func");`
  * 포즈 전체(몸·손)와 손 전용 인식 함수 분리 구조
*** 이벤트 기반 호출 루프

  * `while (pDlg->m_exit.load() == 0)` 루프
  * `WaitForSingleObject(pDlg->hAIStart, INFINITE)`

    * 메인 쓰레드가 `SetEvent(hAIStart)` 호출할 때까지 대기
*** 입력 이미지 준비

  * `dst_img = pDlg->m_cap_img.clone()`
  * 채널 수 4인 경우 `cvtColor(dst_img, dst_img, COLOR_BGRA2BGR)`

    * BGRA → BGR 변환으로 MediaPipe 입력 포맷 정규화
*** MediaPipe 포즈 함수 호출

  * `auto result_pose_mp = func_mp_pose(pybind11::cast(dst_img));`

    * OpenCV `cv::Mat` → pybind11 `array` 캐스팅 후 Python 함수 인자 전달
  * `auto mp_result = result_pose_mp.cast<std::vector<std::vector<double>>>();`

    * Python 반환값을 `std::vector<std::vector<double>>` 로 역캐스팅
    * 3×N 형식 (x 배열, y 배열, z 배열) 구조 가정 가능

**** 4. MediaPipe 결과를 3D 스켈레톤으로 변환·저장

*** 스켈레톤 버퍼 초기화

  * `pDlg->m_sgcp_vtSkeletonMP.clear();`
  * 현재 프레임의 스켈레톤 포인트 벡터 비우기
*** 3D 포인트 생성 로직

  * 조건: `mp_result.size() >= 3` (x, y, z 세 행 이상 존재 시)
  * 루프

    * `for (int k = 0; k < mp_result[0].size(); k++)`
    * `Point3f(mp_result[0][k], mp_result[1][k], mp_result[2][k])`
    * 3D 관절 좌표를 `cv::Point3f` 로 생성 후 `m_sgcp_vtSkeletonMP` 에 `push_back`
*** 의미

  * `m_sgcp_vtSkeletonMP`

    * MediaPipe 출력 3D 스켈레톤(몸·손 관절)의 내부 표현
    * 이후 손 위치/동작 판단 및 gRPC 전송에 공통 사용
  * proto 정의 상 `Point3` 메시지

    * `float x`, `float y`, `float z`
    * MediaPipe 스켈레톤 포인트를 네트워크로 전송하기 위한 공용 데이터 구조 역할

**** 5. 2D 손 위치 추출 및 거친 손 상태 판단

*** 참조 손 위치 계산 함수

  * `std::vector<Point2f> GetRefHL2DPointsMP(std::vector<cv::Point3f> joints)`
  * `joints.size() <= 0` 인 경우 빈 벡터 반환
  * 3D 스켈레톤 각 점을 2D 픽셀 좌표로 변환
  * 왼손·오른손 기준 위치 두 점(인덱스 0: Left, 1: Right) 추출 용도
*** 거친 손 상태 판별 함수

  * `int GetRoughHandStatusFromMP(std::vector<Point3f> mp_pose)`
  * 초기값 `status = -1`

    * 입력 포즈 없음 → 에러 상태 -1 반환
  * 처리 흐름

    * `auto hl = GetRefHL2DPointsMP(mp_pose);`
    * `hl[0]`, `hl[1]` 의 y 좌표와 `READY_LOCATION` 상수 비교
  * 상태 코드

    * `status = 3`

      * 두 손 y ≤ READY_LOCATION
      * 양손 모두 준비선 위 위치
    * `status = 1`

      * 오른손만 READY_LOCATION 위, 왼손은 아래
    * `status = 2`

      * 왼손만 READY_LOCATION 위, 오른손은 아래
    * `status = 0`

      * 양손 모두 READY_LOCATION 아래 (손 내림 상태)

**** 6. 프레임 간 스켈레톤 변화를 이용한 손동작(모션) 상태 추적

*** 함수 시그니처

  * `int GetMotionStatusFromMP(int cur_motion_status, std::vector<cv::Point3f> cur_mp)`
*** 내부 상태 보관

  * `static std::vector<cv::Point3f> prev_mp;`
  * 이전 프레임의 3D 스켈레톤 유지
*** 리셋 처리

  * `cur_motion_status == RESET_MOTION_STATUS` 인 경우

    * `prev_mp.clear()`
    * 그대로 `RESET_MOTION_STATUS` 반환
    * 상태 초기화 및 히스토리 삭제 동작
*** 첫 프레임 처리

  * `prev_mp.size() <= 0` 인 경우

    * `prev_mp = cur_mp`
    * `READY_MOTION_STATUS` 반환
    * 모션 상태 초기값 준비 상태로 설정
*** 손 이동 거리 계산

  * 오른손 기준

    * `Rdev = m_util.Distance(Point2f(prev_mp[16].x - cur_mp[16].x, prev_mp[16].y - cur_mp[16].y));`
  * 왼손 기준

    * `Ldev = m_util.Distance(Point2f(prev_mp[15].x - cur_mp[15].x, prev_mp[15].y - cur_mp[15].y));`
  * 의미

    * MediaPipe 스켈레톤에서 특정 관절 인덱스(15, 16)를 손 대표 포인트로 사용
    * 이전 프레임과의 거리로 손 움직임 크기 측정
*** 급격한 움직임 판정

  * 조건: `Rdev > RAPID_DISTANCE || Ldev > RAPID_DISTANCE`
  * 만족 시 즉시 `RAPID_MOTION_STATUS` 반환
  * 손이 빠르게 크게 이동한 제스처 구간 표시 용도
*** 상태 머신 로직

  * 현재 상태 `cur_motion_status == READY_MOTION_STATUS` 인 경우

    * `int hand_status = GetRoughHandStatusFromMP(cur_mp);`
    * `hand_status > 0` 이면 `motion_status = SPEAK_MOTION_STATUS`
    * 손이 “말하기(표현)”를 시작한 상태로 전이
  * 현재 상태 `cur_motion_status == SPEAK_MOTION_STATUS` 인 경우

    * `prev_mp.size() > 0 && cur_mp.size() > 0` 조건 하에서
    * `motion_status = READY_MOTION_STATUS` 로 복귀
    * 특정 조건 만족 시 동작 종료 후 다시 준비 상태로 돌아가는 구조
*** 마지막 업데이트

  * `prev_mp = cur_mp;`
  * 최신 프레임 스켈레톤을 다음 프레임 비교 기준으로 저장
  * 반환값 `motion_status`

    * READY / SPEAK / RAPID / RESET 등 손동작 상태 코드

**** 7. Optical Flow 와의 결합에 의한 손동작 구간 강화

*** Optical Flow 기반 전체 모션 값

  * `double GetMotionVauleWithOpticalFlow(...)` 계열 함수
  * ROI 내부 특징점 추출(`goodFeaturesToTrack`) 후
  * `calcOpticalFlowPyrLK` 으로 특징점 이동 거리 평균 계산
  * 결과 `avgMotion` 값 출력
*** 손동작 인식 신뢰도 향상

  * MediaPipe 스켈레톤 기반

    * `GetRoughHandStatusFromMP` → 손 유무, 어느 손인지
    * `GetMotionStatusFromMP` → READY/SPEAK/RAPID 등 동작 상태
  * Optical Flow 기반

    * 손 주변 전체 픽셀 수준 움직임 크기
  * 두 정보 결합

    * 손 스켈레톤이 크게 변하고
    * Optical Flow 모션이 일정 임계 이상이거나, 이후 정지 구간으로 떨어지는 시점
    * 의미 있는 손동작(발화 구간, 분절점) 후보 프레임 검출 로직에 사용

**** 8. 손동작 인식 결과의 최종 활용

*** `m_sgcp_vtSkeletonMP`

  * MediaPipe 3D 포즈 결과 저장 버퍼
  * gRPC `Frame` 메시지의 `pose_points` 필드로 매핑
  * 서버 측 문장 인식/손동작 분석 모델의 입력 역할
*** `GetRoughHandStatusFromMP` / `GetMotionStatusFromMP`

  * 클라이언트 내부에서

    * 손 존재 여부
    * 왼손/오른손 구분
    * 동작 상태(준비/발화/급격한 모션 등) 판단
  * 키프레임 전송 조건, UI 정보 표시, 디버깅 로그 등에 활용 가능성

이 구조 기준으로 보면,

*** MediaPipe 역할: 각 프레임에서 3D 스켈레톤(손 포함) 좌표 추출
*** C++/OpenCV 역할: ROI 자르기, Python 호출, Optical Flow 계산
*** 손동작 인식 핵심:

  * 스켈레톤 기반 손 위치·상태(READY/SPEAK/RAPID) 판정
  * Optical Flow 기반 전체 모션 크기와 결합하여 의미 있는 손동작 구간 추출
** gemini-web
*** AIThread 실행 흐름 분석 (CgRPCFileClientDlg::AIThread)

`AIThread`는 메인 로직(비디오 재생 또는 gRPC 전송 스레드)으로부터 영상 데이터가 준비되었다는 신호를 받으면, 내장된 Python 인터프리터를 통해 MediaPipe 로직을 수행하고 결과를 C++ 구조체에 저장합니다.

**** 1. 실행 대기 및 시작 신호 수신 (Wait for Signal)
   AI 스레드는 무한 루프 내에서 `hAIStart` 이벤트가 발생하기를 대기합니다. 이 이벤트는 메인 스레드나 비디오 스레드에서 `SetEvent(hAIStart)`를 호출할 때 활성화됩니다.
   - `WaitForSingleObject`: CPU 자원을 소모하지 않고 대기 상태로 머뭅니다.
   - `pDlg->m_cap_img`: 분석할 이미지가 비어있는지 안전 장치로 확인합니다.

   #+BEGIN_SRC cpp
   // gRPCFileClientDlg.cpp : AIThread 내부 Loop

   while (pDlg->m_exit.load() == 0)
   {
       // hAIStart 이벤트가 SetEvent 될 때까지 대기
       WaitForSingleObject(pDlg->hAIStart, INFINITE);

       // 영상 데이터가 비어있지 않은 경우에만 로직 수행
       if (!pDlg->m_cap_img.empty())
       {
           // ... (이후 로직 진행)
       }
       // ...
   }
   #+END_SRC


**** 2. 이미지 전처리 (Preprocessing)
   원본 이미지를 복제하고, Python 라이브러리(MediaPipe)가 처리하기 적합한 포맷으로 변환합니다.
   - `clone()`: 원본 `m_cap_img`가 다른 스레드(비디오 재생 등)에서 동시에 변경될 수 있으므로, 반드시 깊은 복사를 수행하여 스레드 안전성을 확보합니다.
   - `cvtColor`: OpenCV의 기본인 4채널(BGRA) 이미지인 경우 3채널(BGR)로 변환합니다.

   #+BEGIN_SRC cpp
   // 이미지 깊은 복사
   cv::Mat dst_img = pDlg->m_cap_img.clone();

   // 4채널(투명도 포함)인 경우 3채널로 변환
   if (dst_img.channels() == 4)
       cvtColor(dst_img, dst_img, COLOR_BGRA2BGR);
   #+END_SRC


**** 3. Python 함수 호출 및 추론 (Python Inference)
   `pybind11`을 사용하여 C++의 `cv::Mat`을 Python 객체로 변환하고, 스레드 초기화 시점에 로드해둔 Python 함수(`func_mp_pose`)를 호출합니다.
   - `pybind11::cast(dst_img)`: OpenCV Mat 데이터를 Python `numpy.ndarray`로 자동 변환하여 인자로 전달합니다.
   - `func_mp_pose(...)`: Python 영역(`ACR.mp_detect` 모듈)의 MediaPipe Pose 추론을 실행하고 결과를 반환받습니다.

   #+BEGIN_SRC cpp
   // Python 함수 호출 (이미지 전달 및 추론 실행)
   auto result_pose_mp = func_mp_pose(pybind11::cast(dst_img));
   #+END_SRC


**** 4. 결과 데이터 파싱 (Result Parsing)
   Python 리턴값을 C++ 데이터 타입으로 변환하고, 결과를 저장할 멤버 변수 벡터를 초기화합니다.
   - 반환값 구조: Python List `[[x1, x2...], [y1, y2...], [z1, z2...]]` 형태.
   - `cast<...>`: Python 객체를 C++의 `std::vector<std::vector<double>>` 타입으로 캐스팅합니다.

   #+BEGIN_SRC cpp
   // Python 반환값을 C++ 이중 벡터로 변환
   auto mp_result = result_pose_mp.cast<std::vector<std::vector <double>>>();

   // 기존에 저장된 스켈레톤 데이터 초기화
   pDlg->m_sgcp_vtSkeletonMP.clear();
   #+END_SRC


**** 5. 관절 좌표 저장 (Store Skeleton Data)
   파싱된 좌표 데이터를 순회하며 `Point3f` 객체로 생성하여 최종 저장합니다.
   - `mp_result[0]`: X 좌표 리스트
   - `mp_result[1]`: Y 좌표 리스트
   - `mp_result[2]`: Z 좌표 리스트
   - 위 3개 리스트의 같은 인덱스(`k`)를 조합하여 하나의 3D 좌표(`Point3f`)를 만듭니다.

   #+BEGIN_SRC cpp
   // 데이터 유효성 검사 (X, Y, Z 축 데이터가 모두 있는지)
   if (mp_result.size() >= 3)
   {
       // 랜드마크 개수만큼 반복
       for (int k = 0; k < mp_result[0].size(); k++)
       {
           // x, y, z 좌표를 묶어 Point3f로 저장
           pDlg->m_sgcp_vtSkeletonMP.push_back(
               Point3f(mp_result[0][k], mp_result[1][k], mp_result[2][k])
           );
       }
   }
   #+END_SRC


**** 6. 완료 신호 전송 (Signal Completion)
   모든 처리가 완료되었음을 알리는 이벤트를 발생시킵니다. 이를 통해 `WaitForSingleObject(hAIFinish)`로 대기 중이던 메인 로직(전송 스레드 등)이 깨어나 다음 단계(gRPC 전송 등)를 진행하게 됩니다.

   #+BEGIN_SRC cpp
   // 처리 완료 신호 발생
   SetEvent(pDlg->hAIFinish);
   #+END_SRC
* KeyFrame 알고리즘-1
MFC 소스(gRPCFileClientDlg.cpp 및 HandTurnDetector.hpp)에서 KeyFrame(수어 시작 지점, speakstartflag)을 감지하는 알고리즘은 광학 흐름(Optical Flow) 기반의 모션 감지와 손 궤적 기반의 방향 전환 감지(Turn Detection) 가 결합된 형태입니다.

핵심 조건은 다음과 같습니다:

1. 전제 조건 (Pre-condition):
   - GetRoughHandStatusFromMP: 손이 '준비 상태(Ready)'가 아니어야 합니다(즉, 손이 일정 높이 이상 올라와 있어야 함).
   - GetMotionStatusFromMP: 현재 동작 상태가 SPEAK_MOTION_STATUS(수어 중)이거나 일정 프레임 이상 지났어야 합니다.
2. 핵심 트리거 (Key Trigger) - HandTurnDetector:
   - HandTurnDetector::update 함수가 true를 반환해야 합니다.
   - 알고리즘 상세 (=HandTurnDetector.hpp=):
     - 손의 속도(Velocity) 와 이동 방향(Direction) 을 분석합니다.
     - 조건 1 (방향 전환): 이동 방향의 각도 변화(dtheta)가 angleRadTh_ (기본 30도) 이상이어야 합니다.
     - 조건 2 (감속): 속도가 이전 프레임 대비 speedRatioTh_ (기본 0.8, 즉 20% 감소) 이하로 줄어들어야 합니다. (Turn & Stop/Slow)
     - 조건 3 (최소 속도): 너무 느린 움직임은 무시 (minSpeed_ 이상이어야 함).
   - 즉, "손이 특정 방향으로 빠르게 이동하다가 방향을 30도 이상 꺾으면서 속도가 20% 이상 줄어드는 순간" 을 감지합니다.
3. 보조 조건 (Optical Flow):
   - speakstartflag가 1로 설정되면(Turn 감지 후), 이후 avgMotion(Optical Flow 평균값)이 OPTICALFLOW_THRESH(0.45) 미만으로 떨어지는지 확인하여 '정지(Hold)' 구간을 찾기도 합니다(Segmentation).
* KeyFrame 알고리즘-2
MFC 프로젝트(=gRPCFileClientDlg.cpp=, =HandTurnDetector.hpp=)에서 *KeyFrame(수어 시작 지점, =speakstartflag=)* 을 결정하는 알고리즘은 *1. 동작 상태 전제 조건*, *2. HandTurnDetector (방향 전환 감지)*, *3. Optical Flow (움직임 멈춤 감지)* 세 가지 단계가 복합적으로 작용합니다.
이를 상세히 분석하면 다음과 같습니다.
*** 1. 전제 조건 (Pre-condition)
KeyFrame 감지 로직이 실행되기 위한 기본 조건입니다.

- *손의 위치 상태 (=GetRoughHandStatusFromMP=)*:
  - MediaPipe 스켈레톤 데이터를 기반으로 손이 *준비 상태(Ready Status)*, 즉 몸 아래로 내려가 있는지, 혹은 일정 높이(Y축 =READY_LOCATION=) 이상 올라와 있는지 확인합니다.
  - *조건*: 손이 준비 상태(=0=)가 아니어야 합니다. (즉, 손이 올라와 있어야 함)
- *동작 상태 (=GetMotionStatusFromMP=)*:
  - 현재 동작이 *수어 발화 중(=SPEAK_MOTION_STATUS=)* 이거나, 영상 재생 후 일정 프레임(=MIN_FRAME=) 이상 지났는지 확인합니다.
  - 이는 너무 이른 시점이나 손을 내리고 있을 때의 오탐지를 방지합니다.

*** 2. 핵심: HandTurnDetector (방향 전환 및 감속 감지)
이것이 KeyFrame을 결정하는 가장 중요한 트리거입니다. 손이 단순히 움직이는 것이 아니라, *동작의 의미 있는 변화(Turn)* 가 일어나는 순간을 포착합니다.

- *입력*: 손목(Wrist)의 2D 좌표 (=x=, =y=)와 시간 간격(=dt=).
- *로직 (=HandTurnDetector::update=)*:
  1. *속도(Velocity) 계산*: 현재 위치와 이전 위치의 차이를 시간(=dt=)으로 나누어 속도 벡터를 구합니다.
  2. *속력(Speed) 계산*: 속도 벡터의 크기를 구합니다.
  3. *방향 변화(Angle Change) 계산*:
     - 이전 속도 벡터(=u1=)와 현재 속도 벡터(=u2=)를 단위 벡터로 변환합니다.
     - 두 벡터의 내적(Dot Product)을 통해 사이각(=dtheta=)을 구합니다.
  4. *속도 비율(Speed Ratio) 계산*: 현재 속력 / 이전 속력 비율(=ratio=)을 구합니다.
  5. *판정 조건 (AND 조건)*:
     - *각도 변화 > 30도* (=dtheta > angleRadTh_=): 이동 방향이 30도 이상 꺾여야 합니다.
     - *감속 (속도 줄어듦)* (=ratio < speedRatioTh_=): 속도가 이전 대비 80% 미만으로 줄어야 합니다. (즉, 20% 이상 감속)
     - *최소 속도*: 너무 느린 움직임(노이즈)은 제외합니다.
- *의미*: 손이 빠르게 뻗어나가다가 방향을 바꾸거나 멈칫하는 순간, 즉 *수어 동작의 '획'이 바뀌거나 시작되는 지점*을 의미합니다.

*** 3. 보조: Optical Flow (모션 안정화 확인)
=HandTurnDetector=가 트리거된 후, 추가적으로 영상의 픽셀 변화량(Optical Flow)을 확인하여 최종 전송 시점을 조율하기도 합니다.

- =avgMotion < OPTICALFLOW_THRESH=: 화면 전체의 움직임이 일정 수준 이하로 안정화되었을 때.
- =HandTurnDetector=가 감지되었을 때 =speakstartflag=를 1로 세팅하고, 이후 Optical Flow가 낮아지면(동작이 멈추면) 그때 데이터를 전송하고 =frozenflag=를 설정하여 중복 전송을 막습니다(Debouncing).
- *MFC 코드 상의 흐름*: =cpp     if (Rhit || Lhit) { // HandTurnDetector 감지됨         speakstartflag = 1;     }     else if (speakstartflag && avgMotion < 0.45 && frozenflag == 0) {         // Turn 감지 이후 + 움직임이 잦아들었을 때 -> 최종 전송         EncodeFrame(...);         writer->Write(frame);         frozenflag = 3; // 3프레임 동안 전송 금지     }=

*** 요약: MFC의 KeyFrame 결정 로직

#+begin_quote
*"손이 일정 높이 이상 올라와 있는 상태에서(1), 손목이 30도 이상 방향을 꺾으며 감속하는 순간(2)을 'Turn'으로 감지하고, 그 직후 손의 움직임이 잠시 멈추는 순간(3)을 수어 단어의 시작/끝으로 판단하여 서버로 전송한다."*

#+end_quote

현재 =ksl-cli=에 적용된 로직은 *2번(HandTurnDetector)* 까지는 완벽히 포팅되었으나, *3번(Optical Flow 및 speakstartflag 상태 관리)* 로직은 아직 구현되지 않고, Turn 감지 즉시 전송하도록 되어 있습니다. 더 정확한 타이밍을 원하신다면 3번 로직까지 추가해야 합니다.
* KeyFrame -3
--------------

*** KeyFrame 결정 알고리즘 상세 분석
이 알고리즘은 *"준비된 상태에서(Pre-condition) -> 손이 꺾이며 감속하는 순간을 포착하고(Turn Detection) -> 동작이 멈출 때(Optical Flow)"* 를 KeyFrame으로 판정합니다.

**** 1단계: 전제 조건 및 상태 확인 (Pre-condition & State Check)
가장 먼저, 현재 프레임이 분석할 가치가 있는 상태인지 확인합니다. 손이 너무 아래에 있거나(차렷 자세), 아직 분석 준비가 안 된 경우를 걸러냅니다.

- *핵심 코드 (=gRPCFileClientDlg.cpp=)*:

  #+begin_src cpp
  // 1. 손 상태 확인 (0: Ready/Below Threshold, 1/2/3: Hand Up)
  int hand_status = pDlg->GetRoughHandStatusFromMP(pDlg->m_sgcp_vtSkeletonMP);

  // 2. 동작 상태 업데이트 (READY_MOTION_STATUS vs SPEAK_MOTION_STATUS)
  motion_status = pDlg->GetMotionStatusFromMP(motion_status, pDlg->m_sgcp_vtSkeletonMP);

  // 3. 조건 검사: 손이 올라와 있고(hand_status != 0), 수어 중이거나 일정 시간 경과 시
  if (hand_status != 0 && (motion_status == SPEAK_MOTION_STATUS || frame_count > MIN_FRAME)) {
      // ... 다음 단계 진입 ...
  }
  #+end_src

- *예제 데이터 (Pseudo Data)*:

  - *상황 A (무시)*:
    - =LeftWrist.y = 0.8= (화면 하단, =READY_LOCATION=0.65=보다 큼 → 아래에 있음)
    - =RightWrist.y = 0.8=
    - *결과*: =hand_status = 0= (Ready). *Stop*. (다음 단계 진행 안 함)
  - *상황 B (진입)*:
    - =RightWrist.y = 0.4= (화면 상단, 가슴 높이)
    - *결과*: =hand_status = 1= (Right Hand Up). *Pass*. (2단계로 진입)

--------------

**** 2단계: HandTurnDetector - 방향 전환 및 감속 감지 (The Trigger)
손이 움직이다가 방향을 급격히 바꾸거나(Turn), 멈추기 위해 감속하는 순간을 '동작의 획'으로 간주하여 감지합니다. 이 단계가 *=speakstartflag=를 켜는 트리거*입니다.

- *핵심 코드 (=HandTurnDetector.hpp=)*:

  #+begin_src cpp
  // 벡터 내적을 통한 각도 계산
  float dot = u1.x * u2.x + u1.y * u2.y; // u1:이전단위벡터, u2:현재단위벡터
  float dtheta = std::acos(dot);         // 사이각 (라디안)

  // 속도 비율 계산 (현재속도 / 이전속도)
  float ratio = speed / (prevSpeed + 1e-6f);

  // 판정: 각도가 30도(0.52rad) 이상 꺾이고 AND 속도가 80% 미만으로 줄었을 때
  if (dtheta > angleRadTh_ && ratio < speedRatioTh_) {
      detected = true;
  }
  #+end_src

- *예제 데이터 (Pseudo Data)*:

  - *입력*:
    - 이전 속도 벡터 (=prev_vel=): =(10, 0)= (오른쪽으로 빠르게 이동 중, 속력 10)
    - 현재 속도 벡터 (=curr_vel=): =(5, 5)= (대각선 위로 이동 중, 속력 약 7.07)
  - *계산*:
    1. *각도 (=dtheta=)*: =(1,0)=과 =(0.707, 0.707)=의 사이각 = *45도* (Turn 발생! => 30도= 조건 만족)
    2. *속도 비율 (=ratio=)*: =7.07 / 10.0= = *0.707* (70% 수준으로 감속됨! =< 0.8= 조건 만족)
  - *결과*: *=detected = true=*.
  - *Action*: =speakstartflag = 1= 로 설정. (전송 대기 상태 돌입)

--------------

**** 3단계: Optical Flow - 정지 확인 및 최종 전송 (Stabilization & Send)
Turn 감지(2단계) 직후 바로 보내지 않고, 손이나 몸의 잔여 움직임이 잦아들어 *이미지가 선명해지는 순간(Stop)* 을 기다렸다가 전송합니다.

- *핵심 코드 (=gRPCFileClientDlg.cpp=)*:

  #+begin_src cpp
  // 전체 화면(ROI)의 평균 움직임 양 계산 (Optical Flow)
  double avgMotion = pDlg->GetMotionVauleWithOpticalFlow(roi);

  // 조건: Turn이 감지되었었고(speakstartflag), 움직임이 안정화(avgMotion < 0.45) 되었으며, 쿨타임(frozenflag)이 아닐 때
  if (speakstartflag && avgMotion < OPTICALFLOW_THRESH && frozenflag == 0) {
      Holdcnt++;

      // ★ 최종 전송 ★
      EncodeFrame(roi, session_id, frameIndex, 1, ...);
      writer->Write(frame);

      // 중복 전송 방지 쿨타임 설정
      frozenflag = OPTICAL_FLOW_HOLD_FRAME; // 3프레임
  }
  #+end_src

- *예제 데이터 (Pseudo Data)*:

  - *Frame 10 (Turn 감지 직후)*:
    - =speakstartflag = 1= (2단계에서 설정됨)
    - =avgMotion = 2.5= (아직 손이 떨리고 있음, => 0.45=)
    - *결과*: 전송 보류.
  - *Frame 12*:
    - =speakstartflag = 1=
    - =avgMotion = 0.1= (손이 딱 멈춤! =< 0.45=)
    - =frozenflag = 0= (쿨타임 끝남)
  - *결과*: *전송 실행 (Send Frame)*.
  - *Action*: =frozenflag = 3= 설정. (Frame 13, 14, 15는 전송 안 함)
* KeyFrame  전송 pseudo code
=mfc_client=가 비디오 스트림에서 *KeyFrame을 선별하여 전송하는 전체 과정*을 Pseudo Code(의사 코드)로 정리했습니다.

이 코드는 *"어떤 조건에서 프레임이 전송되고, 어떤 상황에서 무시되는지"*를 명확하게 보여줍니다.

#+begin_src python
# 초기 상태 변수 설정
bool speakStartFlag = false;    # 수어 동작 시작 감지 여부 (Latching)
int frozenCounter = 0;          # 전송 후 대기(Cooldown) 카운터
const int COOLDOWN_FRAMES = 3;  # KeyFrame 전송 후 무시할 프레임 수
const float MOTION_THRESH = 0.45; # 정지 상태로 판단할 모션 임계값

# 비디오 스트림 처리 루프
WHILE (비디오_재생중 AND 종료_아님) DO:

    # 1. 데이터 획득
    Frame currentFrame = VideoCapture.read();
    List<Point3D> skeleton = Run_MediaPipe_AI(currentFrame); # 3D 관절 추출

    # [조건 1] 스켈레톤 데이터가 유효한가?
    IF (skeleton.isEmpty()) THEN:
        CONTINUE; # 다음 프레임으로 (데이터 없음)

    # [조건 2] 손이 "준비(Ready)" 상태가 아닌가? (즉, 손이 올라와 있는가?)
    int handStatus = GetHandStatus(skeleton); # 0:Ready, 1:Right, 2:Left, 3:Both

    IF (handStatus == 0) THEN:
        CONTINUE; # 다음 프레임으로 (의미 없는 대기 동작)

    # -----------------------------------------------------------
    # 여기서부터는 유효한 동작 구간임
    # -----------------------------------------------------------

    # 3. 움직임 분석
    float avgMotion = CalculateOpticalFlow(prevFrame, currentFrame); # 움직임 양 계산
    bool isTurnDetected = DetectHandTurnOrDeceleration(skeleton);    # 회전/감속 감지

    # 수어 발화 시작 감지 (한 번 true가 되면 유지됨)
    IF (isTurnDetected) THEN:
        speakStartFlag = true;

    # [조건 3 & 4] 전송 결정 로직 (핵심)
    # - 동작이 시작되었고 (speakStartFlag)
    # - 손이 멈췄거나 매우 느리며 (avgMotion < THRESH)
    # - 쿨다운 시간이 끝났을 때 (frozenCounter == 0)
    IF (speakStartFlag == true AND avgMotion < MOTION_THRESH AND frozenCounter == 0) THEN:

        # >>> KEYFRAME 전송 <<<
        GrpcClient.Send(currentFrame, skeleton);

        # 전송 후 쿨다운 설정 (중복 전송 방지)
        frozenCounter = COOLDOWN_FRAMES;

    # 4. 상태 관리
    IF (frozenCounter > 0) THEN:
        frozenCounter = frozenCounter - 1; # 매 프레임마다 카운터 감소

END WHILE
#+end_src

*** 요약: 데이터 전송 흐름
1. *AI 분석*: 매 프레임마다 MediaPipe를 돌려 관절(Skeleton)을 얻습니다.
2. *필터링*: 관절 데이터가 없거나, 손이 허리춤(Ready 상태)에 있으면 즉시 무시합니다.
3. *상태 감지*: 손을 비틀거나 속도를 줄이는 동작이 감지되면 =speakStartFlag=를 켜서 "지금부터 수어 중이다"라고 표시합니다.
4. *KeyFrame 포착 (AND 조건)*:
   - 수어 중(=speakStartFlag=)이면서,
   - 손이 순간적으로 멈췄고(=avgMotion= 낮음),
   - 방금 전송한 적이 없다면(=frozenCounter == 0=),
   - *그제서야 서버로 프레임을 전송*합니다.
5. *쿨다운*: 전송 직후 =frozenCounter=를 3으로 설정하여, 향후 3 프레임 동안은 손이 계속 멈춰 있어도 전송하지 않습니다. (중복 방지)

* mediapipe 판단 과정 예제
** KeyFrame 결정 알고리즘의 상세 설명
**주요 목적:** 사용자의 수어 동작 시퀀스에서 의미 있는 시작점, 정지 지점 또는 중요한 자세 변화를 나타내는 프레임을 'KeyFrame'으로 식별하여 gRPC 서버로 전송합니다.

**1. 초기화 및 비디오 프레임 처리 루프 (Within =SequenceClient::SendFrames=):**
   - **비디오 로드:** 사용자가 지정한 비디오 파일을 =cv::VideoCapture=로 엽니다.
   - **프레임 루프:** 비디오의 시작 프레임부터 종료 프레임까지 순회하면서 각 프레임에 대해 다음 단계를 반복합니다.
   - **ROI 추출:** 현재 프레임(=img=)에서 미리 정의된 ROI(=pDlg->m_roi=)를 추출하여 =roi=(=cv::Mat=) 변수에 저장합니다. 이 =roi=는 주로 손과 상체 영역을 포함하며, 이후 모든 분석에 사용됩니다.
   - **ROI 리사이즈:** 필요한 경우 =roi=를 특정 크기(예: 320px 너비)로 리사이즈하여 =pDlg->m_cap_img=에 저장합니다. 이는 AI 처리의 입력 크기를 표준화합니다.

**2. AI (MediaPipe Pose) 처리 (Delegated to =AIThread=):**
   - **AI 스레드 트리거:** =SetEvent(pDlg->hAIStart)=를 호출하여 =AIThread=가 현재 =pDlg->m_cap_img=에 있는 프레임을 처리하도록 지시합니다.
   - **AI 스레드 대기:** =WaitForSingleObject(pDlg->hAIFinish, INFINITE)=를 통해 =AIThread=의 작업이 완료될 때까지 대기합니다.
   - **결과 획득:** =AIThread=는 =pybind11=을 통해 Python =ACR.mp_detect= 모듈의 =mediapipe_pose_func=를 호출하여 MediaPipe Pose 감지를 수행하고, 감지된 3D 포즈 키포인트(스켈레톤 데이터)를 =pDlg->m_sgcp_vtSkeletonMP=에 저장합니다. 이 데이터는 KeyFrame 결정의 핵심 입력 중 하나입니다.

**3. KeyFrame 판단을 위한 신호 추출:**

   - **손 상태 추정 (=pDlg->GetRoughHandStatusFromMP=):**
     - =pDlg->m_sgcp_vtSkeletonMP= (MediaPipe 포즈 키포인트)를 입력으로 받아 손의 대략적인 상태를 반환합니다.
     - 반환 값: =-1=(오류), =0=(준비 상태), =1=(오른손 사용), =2=(왼손 사용), =3=(양손 사용).
     - =hand_status != 0=인 경우에만 KeyFrame 후보로 고려됩니다. 즉, 손이 감지되고 특정 역할을 하고 있을 때만 중요하게 봅니다.

   - **움직임 상태 추정 (=pDlg->GetMotionStatusFromMP=):**
     - 현재 움직임 상태(=cur_motion_status=)와 =pDlg->m_sgcp_vtSkeletonMP=를 입력으로 받아 새로운 움직임 상태를 반환합니다.
     - 주로 손목 키포인트(=prev_mp[16]= (오른쪽), =prev_mp[15]= (왼쪽))의 이전 프레임 대비 현재 프레임에서의 위치 변화(=Rdev=, =Ldev=)를 기반으로 판단합니다.
     - 변화가 =RAPID_DISTANCE= 임계값보다 크면 =RAPID_MOTION_STATUS= (빠른 움직임)를 반환합니다.
     - =READY_MOTION_STATUS= (준비 단계)에서 =hand_status > 0=이 되면 =SPEAK_MOTION_STATUS= (수어 단계)로 전환합니다.
     - =motion_status=는 수어 동작의 전반적인 맥락을 제공합니다.

   - **옵티컬 플로우 기반 움직임 값 계산 (=pDlg->GetMotionVauleWithOpticalFlow=):**
     - =roi= (관심 영역)를 입력으로 받아 이전 프레임과의 옵티컬 플로우(Optical Flow)를 계산하여 ROI 내의 평균 움직임 값(=avgMotion=)을 반환합니다.
     - =cv::goodFeaturesToTrack=으로 특징점을 찾고, =cv::calcOpticalFlowPyrLK=를 사용하여 특징점의 움직임을 추적합니다.
     - =avgMotion= 값이 낮을수록 해당 영역의 움직임이 적다는 것을 의미하며, 정지 상태를 판단하는 데 사용됩니다.

   - **손 회전/방향 전환 감지 (=HandTurnDetector::update=):**
     - =pDlg->m_Rdetector= (오른손) 및 =pDlg->m_Ldetector= (왼손) 인스턴스의 =update= 메서드를 호출합니다.
     - =update= 메서드는 손의 위치(=pDlg->m_sgcp_vtSkeletonMP[16]= 또는 =pDlg->m_sgcp_vtSkeletonMP[15]=)와 시간 차이(=dt=)를 입력받아, 손의 **"방향 전환 + 감속"**이 동시에 발생했는지 여부를 =bool= 값으로 반환합니다 (=Rhit=, =Lhit=).
     - =HandTurnDetector=는 다음 두 가지 조건을 모두 만족할 때 =true=를 반환합니다:
       1.  **각도 변화:** 이전 속도 벡터와 현재 속도 벡터 간의 각도 변화가 =angleRadTh_= (기본 30도)를 초과할 때 (방향 전환).
       2.  **속도 비율:** 현재 속도가 이전 속도에 대한 =speedRatioTh_= (기본 0.8)보다 작을 때 (감속).
     - =Rhit= 또는 =Lhit=이 =true=이면 =RTurncnt= 또는 =Lturncnt=를 증가시키고 =speakstartflag=를 =1=로 설정합니다. 이는 수어 동작의 시작점으로 간주될 수 있는 중요한 포인트입니다.

**4. KeyFrame 최종 결정 로직:**

   - 위에서 추출된 모든 신호를 종합하여 KeyFrame을 최종적으로 결정합니다.
   - **조건 1: 손 감지 (=hand_status != 0=)**
     - 먼저 손이 감지되어야 KeyFrame 후보로 고려됩니다.
   - **조건 2: 수어 동작 시작 신호 (=speakstartflag=)**
     - =speakstartflag=는 =HandTurnDetector=에 의해 손의 방향 전환 및 감속이 감지되었을 때 =1=로 설정됩니다. 이는 수어 동작이 시작되었을 가능성을 나타냅니다.
   - **조건 3: 낮은 움직임 값 (=avgMotion < OPTICALFLOW_THRESH=)**
     - ROI 내의 평균 움직임 값(=avgMotion=)이 미리 정의된 =OPTICALFLOW_THRESH= 임계값보다 작아야 합니다. 이는 손이나 팔이 상대적으로 정지된 상태에 있음을 의미합니다.
   - **조건 4: 중복 방지 (=frozenflag == 0=)**
     - =frozenflag=는 이전에 KeyFrame이 전송된 경우 일정 시간 동안(=OPTICAL_FLOW_HOLD_FRAME= 프레임 동안) 추가적인 KeyFrame 전송을 방지하기 위한 플래그입니다. =frozenflag=가 =0=이어야만 KeyFrame으로 전송될 수 있습니다. KeyFrame 전송 후 =frozenflag=는 =OPTICAL_FLOW_HOLD_FRAME= 값으로 설정되고, 매 프레임마다 감소하여 =0=이 될 때까지 새로운 KeyFrame 전송을 억제합니다.

   - **모든 조건 만족 시 KeyFrame으로 전송:**
     - =if (speakstartflag && avgMotion < OPTICALFLOW_THRESH && frozenflag == 0)= 이 조건이 =true=가 되면, 현재 프레임은 KeyFrame으로 간주됩니다.
     - =Holdcnt=를 증가시킵니다.
     - 현재 =roi=, =session_id=, =frameIndex=, =flag= (1), =pDlg->m_sgcp_vtSkeletonMP=를 사용하여 =vision::raw::v1::Frame= 객체를 인코딩합니다 (=EncodeFrame=).
     - 인코딩된 =Frame=을 gRPC =writer->Write(frame)=를 통해 서버로 스트리밍합니다.
     - =frozenflag=를 =OPTICAL_FLOW_HOLD_FRAME=으로 설정하여 다음 =OPTICAL_FLOW_HOLD_FRAME= 동안 KeyFrame 중복 전송을 방지합니다.

이 알고리즘은 MediaPipe를 통해 추출된 정밀한 포즈 데이터와 OpenCV의 옵티컬 플로우, 그리고 수어 동작의 특성을 고려한 커스텀 손 움직임 감지 (=HandTurnDetector=)를 결합하여, 동영상에서 의미 있는 수어 KeyFrame을 효율적으로 식별하고 전송하도록 설계되었습니다.
`HandTurnDetector.hpp` 파일의 실제 코드를 가지고 있지 않으므로, 이 알고리즘이 이름과 문맥상 수행할 것으로 예상되는 *가설적인 실행 과정*을 구체적인 숫자를 들어 설명해 드리겠습니다. 실제 구현은 다를 수 있습니다.

`HandTurnDetector`의 목표는 MediaPipe로 추출된 손의 3D 스켈레톤 데이터를 분석하여 *손의 움직임 방향 전환*이나 *속도 변화(감속)*와 같은 주요 분절점(키프레임)을 감지하는 것입니다. 이를 위해 이전 프레임의 손 상태와 현재 프레임의 손 상태를 비교합니다.

---

**`HandTurnDetector` 알고리즘 가설적 실행 과정 (예시)**

가장 단순한 예시로, `HandTurnDetector`가 손목(`wrist`)과 검지손가락 끝(`index_tip`) 두 지점만을 사용하여 손의 '방향'을 나타내는 벡터를 만들고, 이 벡터의 **각도 변화**를 감지한다고 가정해 보겠습니다.

**내부 상태 변수 (가정):**

** KeyFrame 결정 알고리즘의 상세 설명2
**주요 목적:** 사용자의 수어 동작 시퀀스에서 의미 있는 시작점, 정지 지점 또는 중요한 자세 변화를 나타내는 프레임을 'KeyFrame'으로 식별하여 gRPC 서버로 전송합니다.

**1. 초기화 및 비디오 프레임 처리 루프 (Within =grpc_client_::SequenceClient::SendFrames=):**
   - **비디오 로드:**
     #+begin_src cpp
     VideoCapture cap(pDlg->m_util.StringToChar(file_str));
     #+end_src
   - **프레임 루프:**
     #+begin_src cpp
     while (pDlg->m_exit.load() == false)
     {
         cap >> img;
         int frameIndex = (int)cap.get(cv::CAP_PROP_POS_FRAMES);
         if (img.empty() || frameIndex > _ttoi(strs2[1])) break;
         // ...
     }
     #+end_src
   - **ROI 추출 및 시각화:**
     #+begin_src cpp
     auto cap_img = img.clone();
     rectangle(cap_img, pDlg->m_roi, Scalar(0, 255, 0), 3); // UI에 ROI 사각형 그리기
     // ...
     cv::Mat roi = img(pDlg->m_roi).clone(); // 실제 ROI 데이터 추출
     #+end_src
   - **ROI 리사이즈:**
     #+begin_src cpp
     if (roi.cols >= 320)
     {
         cv::Mat resize_roi_img;
         double src_x = static_cast<double>(roi.cols);
         double src_y = static_cast<double>(roi.rows);
         double target_x = 320.0;
         double scale = target_x / src_x;
         double dst_x_rate = scale;
         double dst_y_rate = scale;
         cv::resize(roi, resize_roi_img, cv::Size(), dst_x_rate, dst_y_rate, cv::INTER_LINEAR);
         pDlg->m_cap_img = resize_roi_img;
     }
     else pDlg->m_cap_img = roi;
     #+end_src

**2. AI (MediaPipe Pose) 처리 (Delegated to =CgRPCFileClientDlg::AIThread=):**
   - **AI 스레드 트리거 및 대기 (from =SequenceClient::SendFrames=):**
     #+begin_src cpp
     SetEvent(pDlg->hAIStart);
     WaitForSingleObject(pDlg->hAIFinish, INFINITE); //Wait for results
     #+end_src
   - **AI 스레드 내부 로직 (from =CgRPCFileClientDlg::AIThread=):**
     #+begin_src cpp
     pybind11::scoped_interpreter guard{};
     try {
         auto exampleModule4 = pybind11::module_::import("ACR.mp_detect");
         auto func_mp_pose = exampleModule4.attr("mediapipe_pose_func");
         // ...
         while (pDlg->m_exit.load() == 0)
         {
             WaitForSingleObject(pDlg->hAIStart, INFINITE);
             if (!pDlg->m_cap_img.empty())
             {
                 cv::Mat dst_img = pDlg->m_cap_img.clone();
                 if (dst_img.channels() == 4) cvtColor(dst_img, dst_img, COLOR_BGRA2BGR);
                 auto result_pose_mp = func_mp_pose(pybind11::cast(dst_img)); // Python MediaPipe 함수 호출
                 auto mp_result = result_pose_mp.cast<std::vector<std::vector <double>>>();

                 pDlg->m_sgcp_vtSkeletonMP.clear();

                 if (mp_result.size() >= 3)
                 {
                     for (int k = 0; k < mp_result[0].size(); k++)
                     {
                         pDlg->m_sgcp_vtSkeletonMP.push_back(Point3f(mp_result[0][k], mp_result[1][k], mp_result[2][k]));
                     }
                 }
             }
             SetEvent(pDlg->hAIFinish);
         }
     } catch (py::error_already_set& e) {
         std::cout << e.what() << std::endl;
     }
     #+end_src

**3. KeyFrame 판단을 위한 신호 추출 (Within =SequenceClient::SendFrames= 및 헬퍼 함수):**

   *   **손 상태 추정 (=pDlg->GetRoughHandStatusFromMP=):**
     #+begin_src cpp
     int hand_status = pDlg->GetRoughHandStatusFromMP(pDlg->m_sgcp_vtSkeletonMP); //손의 위치로 준비, 우세손, 비우세손, 양손 결정
     #+end_src
     #+begin_src cpp
     // CgRPCFileClientDlg::GetRoughHandStatusFromMP 구현
     int CgRPCFileClientDlg::GetRoughHandStatusFromMP(std::vector<Point3f> mp_pose)
     {
         int status = -1; // -1:error, 0:ready, 1:right hand, 2:left hand, 3:all hands
         if (mp_pose.size() <= 0) return -1;
         auto hl = GetRefHL2DPointsMP(mp_pose); // Hand Location 2D Points
         if (hl[1].y <= READY_LOCATION && hl[0].y <= READY_LOCATION) status = 3;
         else if (hl[1].y <= READY_LOCATION && hl[0].y > READY_LOCATION) status = 1;
         else if (hl[1].y > READY_LOCATION && hl[0].y <= READY_LOCATION) status = 2;
         else status = 0;
         return status;
     }
     #+end_src

   *   **움직임 상태 추정 (=pDlg->GetMotionStatusFromMP=):**
     #+begin_src cpp
     motion_status = pDlg->GetMotionStatusFromMP(motion_status, pDlg->m_sgcp_vtSkeletonMP); //준비단계인지 수어 단계인지 구분
     #+end_src
     #+begin_src cpp
     // CgRPCFileClientDlg::GetMotionStatusFromMP 구현
     int CgRPCFileClientDlg::GetMotionStatusFromMP(int cur_motion_status, std::vector<cv::Point3f> cur_mp)
     {
         static std::vector<cv::Point3f> prev_mp;
         if (cur_motion_status == RESET_MOTION_STATUS) { prev_mp.clear(); return RESET_MOTION_STATUS; }
         int motion_status = cur_motion_status;
         if (prev_mp.size() <= 0) { prev_mp = cur_mp; return READY_MOTION_STATUS; }
         auto Rdev = m_util.Distance(cv::Point2f(prev_mp[16].x - cur_mp[16].x, prev_mp[16].y - cur_mp[16].y));
         auto Ldev = m_util.Distance(cv::Point2f(prev_mp[15].x - cur_mp[15].x, prev_mp[15].y - cur_mp[15].y));
         // ...
         if (Rdev > RAPID_DISTANCE || Ldev > RAPID_DISTANCE) return RAPID_MOTION_STATUS;
         if (cur_motion_status == READY_MOTION_STATUS) {
             int hand_status = GetRoughHandStatusFromMP(cur_mp);
             if (hand_status > 0) motion_status = SPEAK_MOTION_STATUS;
         } else if (cur_motion_status == SPEAK_MOTION_STATUS) {
             if (prev_mp.size() > 0 && cur_mp.size() > 0) {
                 motion_status = READY_MOTION_STATUS; // 이 부분은 로직에 따라 변경될 수 있음
             }
         }
         prev_mp = cur_mp;
         return motion_status;
     }
     #+end_src

   *   **옵티컬 플로우 기반 움직임 값 계산 (=pDlg->GetMotionVauleWithOpticalFlow=):**
     #+begin_src cpp
     double avgMotion = pDlg->GetMotionVauleWithOpticalFlow(roi);
     #+end_src
     #+begin_src cpp
     // CgRPCFileClientDlg::GetMotionVauleWithOpticalFlow(cv::Mat frame) 구현
     double CgRPCFileClientDlg::GetMotionVauleWithOpticalFlow(cv::Mat frame)
     {
         static cv::Mat prev;
         // ...
         if (!prev.empty()) avgMotion = GetMotionVauleWithOpticalFlow(prev, frame, rect); // 다른 오버로드 호출
         prev = frame.clone();
         return avgMotion;
     }

     // CgRPCFileClientDlg::GetMotionVauleWithOpticalFlow(cv::Mat prev, cv::Mat frame, cv::Rect rect) 구현
     double CgRPCFileClientDlg::GetMotionVauleWithOpticalFlow(cv::Mat prev, cv::Mat frame, cv::Rect rect)
     {
         cv::Mat prevGray, gray;
         cv::cvtColor(prev, prevGray, cv::COLOR_BGR2GRAY);
         cv::cvtColor(frame, gray, cv::COLOR_BGR2GRAY);
         // ...
         std::vector<cv::Point2f> prevPts;
         cv::goodFeaturesToTrack(prevGray, prevPts, 300, 0.01, 7); // 특징점 추출
         // ...
         cv::calcOpticalFlowPyrLK(prevGray, gray, prevPts, nextPts, status, err); // 옵티컬 플로우 계산
         // ... (움직임 벡터 길이 합산 및 평균 계산)
         return avgMotion;
     }
     #+end_src

   *   **손 회전/방향 전환 감지 (=HandTurnDetector::update=):**
     #+begin_src cpp
     bool Rhit = pDlg->m_Rdetector.update({ pDlg->m_sgcp_vtSkeletonMP[16].x, pDlg->m_sgcp_vtSkeletonMP[16].y }, dt);
     bool Lhit = pDlg->m_Ldetector.update({ pDlg->m_sgcp_vtSkeletonMP[15].x, pDlg->m_sgcp_vtSkeletonMP[15].y }, dt);
     #+end_src
     #+begin_src cpp
     // HandTurnDetector::update 구현 (HandTurnDetector.hpp)
     bool HandTurnDetector::update(const cv::Point2f& pos, double dt)
     {
         bool detected = false;
         if (!hasPrevPos_) { /* ... */ } // 초기화 로직
         cv::Point2f vel;
         // ... (속도 계산) ...
         if (hasPrevVel_) {
             float prevSpeed = std::sqrt(prevVel_.x * prevVel_.x + prevVel_.y * prevVel_.y);
             if (prevSpeed > minSpeed_)
             {
                 cv::Point2f u1 = prevVel_ * (1.0f / prevSpeed);
                 cv::Point2f u2 = vel * (1.0f / (speed + 1e-6f));
                 float dot = u1.x * u2.x + u1.y * u2.y;
                 float dtheta = std::acos(dot); // 각도 변화 (라디안)
                 float ratio = speed / (prevSpeed + 1e-6f); // 속도 비율
                 if (dtheta > angleRadTh_ && ratio < speedRatioTh_) { // 방향 전환 + 감속 조건
                     detected = true;
                 }
             }
         }
         // ... (다음 프레임을 위한 상태 저장) ...
         return detected;
     }
     #+end_src
     #+begin_src cpp
     // Rhit 또는 Lhit이 true이면
     if (Rhit) { RTurncnt++; speakstartflag = 1; }
     else if (Lhit) { Lturncnt++; speakstartflag = 1; }
     #+end_src

**4. KeyFrame 최종 결정 로직 (Within =SequenceClient::SendFrames=):**

   *   **복합 조건 확인:**
     #+begin_src cpp
     if (hand_status != 0) // 조건 1: 손 감지
     {
         // ... (avgMotion, Rhit, Lhit 계산) ...

         if (speakstartflag && avgMotion < OPTICALFLOW_THRESH && frozenflag == 0) // 조건 2,3,4
         {
             Holdcnt++;

             Frame frame;
             EncodeFrame(roi, session_id, frameIndex, 1, pDlg->m_sgcp_vtSkeletonMP,&frame); // KeyFrame 인코딩
             writer->Write(frame); // gRPC 서버로 전송

             frozenflag = OPTICAL_FLOW_HOLD_FRAME; // 중복 방지 플래그 설정
         }
     }
     if (frozenflag > 0) frozenflag--; // frozenflag 감소
     #+end_src

이 알고리즘은 MediaPipe를 통해 추출된 정밀한 포즈 데이터와 OpenCV의 옵티컬 플로우, 그리고 수어 동작의 특성을 고려한 커스텀 손 움직임 감지 (=HandTurnDetector=)를 결합하여, 동영상에서 의미 있는 수어 KeyFrame을 효율적으로 식별하고 전송하도록 설계되었습니다.
